{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory of Regression and Regularization\n",
    "\n",
    "### Data Science 350\n",
    "### Stephen F Elston\n",
    "\n",
    "In this notebook we will explore the mathematical basis of linear statistical models. The emphasis is on ubiquitious problem of **model overfitting** or **model over-parameterizaton**. \n",
    "\n",
    "Overfitting or over-parameterizatio of machine learning models arrises in any case where the number of model parameters exceeds the effective dimensions of the feature set. This is most often the result of linear dependency between the features. However, using too complex a model can lead to similar problems. In the extreme case, imagine a model with as many free parameters as training cases. This model might fit the training data perfectly, but will show unstable and unexpected results when used for any other data. In machine learning terminology, we say that such an unstable model does not **generalize**. \n",
    "\n",
    "Many methods have been developed and continue to be developed to deal with over-paramterized or **ill-posed** machine learning models. In particular, in this notebook we will explore three methods for stabalizing over-parameterized models. \n",
    "\n",
    "- Stepwise regression, wherein features are eliminated from an over-parameterized model in a stepwise fashon.\n",
    "- Using a mathematical **regularization** technique, known as singular value decomposision, to determine the number of meaningful components for a model. \n",
    "- Using **regularizaton** methods known as ridge regression and lasso regression to stabalize over-parameterized models. \n",
    "\n",
    "![](img/Extrapolation.png)\n",
    "<center> **Warning!! Extrapolation can be dangerious!!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note:** To run the code in this note book you must have installed the following packages:\n",
    "- HistData\n",
    "- dplyr\n",
    "- ggplot2\n",
    "- gridExtra\n",
    "- MASS\n",
    "- glmnet\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stepwise Regression\n",
    "\n",
    "In this section we will work through an example of stepwise regression using the Gaulton family height data. The goal of the model is to predict the hight of adult childern given informaiton on the height of their parents. \n",
    "\n",
    "### Preparing the data\n",
    "\n",
    "As a first step we will create a data set for just the adult male childern. The code in the cell below performs the following operations:\n",
    "\n",
    "- Subset the data to just male adult childern.\n",
    "- Compute two new feaures, the heights of the parents squared.\n",
    "- Zscore scale the features.\n",
    "\n",
    "Execute this code to prepare the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require(HistData)\n",
    "require(dplyr)\n",
    "males = GaltonFamilies[GaltonFamilies$gender == 'male',]\n",
    "males.ext = males[, c('mother', 'father', 'childHeight')]\n",
    "males.ext = mutate(males.ext, mother.sqr = mother^2, father.sqr = father^2)\n",
    "males.ext[, c('mother', 'father', 'mother.sqr', 'father.sqr')] = \n",
    "        lapply(males.ext[, c('mother', 'father', 'mother.sqr', 'father.sqr')], \n",
    "               scale)\n",
    "str(males.ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "### Computing a model with all features\n",
    "\n",
    "As a first step, let's compute a model for the hight of the adult male childern using all available features. Execute the code in the cell below to compute this model, and print and plot evaluation information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm.males = lm(childHeight ~ ., data = males.ext)\n",
    "summary(lm.males)\n",
    "plot(lm.males)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the summary that this model is over-parameterized. Only the intercept is significant. In other words, we are computing the average value of the lable (childHeight), but nothing more. Examination of the residual plots shows them to be mostly well behaved, except with a bit of curvature in the standardized residual plot. \n",
    "\n",
    "### Apply stepwise regression\n",
    "\n",
    "Stepwise regression using model performance metrics to prune the number of features in a model. The steps can be forward, wherein features are added one at a time in order of importance, until a point of diminished return is reachec. Or, the steps can be backward, wherein a model using all features is puruned one feature at a time in reverse order of importance. It is also possible to step in both directions. In practice, either backward steps of using both directions are used, since forward steps have a tendancy to get stuck at poor solutions. \n",
    "\n",
    "A significant issue with stepwise regression is to choose a performance metric. Many commonly used error metrics like RMSE will natually get better as we add more model parameters. Consequently the **Akaike information criterion** (AIC) is often used. We can write the AIC as:\n",
    "\n",
    "$$AIC = 2 k - 2 ln(\\hat{L})\\\\\n",
    "where\\\\\n",
    "\\hat{L} = the\\ likelihood\\ given\\ the\\ fitted\\ model\\ parmaters\\ \\hat\\theta = p(x| \\hat\\theta)\\\\\n",
    "x = observed\\ data\\\\\n",
    "k = number\\ of\\ model\\ parameters$$\n",
    "\n",
    "In words, the AIC is the model log-likelihood adjusted for the number of model parameters. The objective is to minimize the AIC. \n",
    "\n",
    "The quantity $- 2 ln(p(x| \\hat\\theta))$ is sometimes reffered to as the **deviance** of the model. Deviance is a measure of the relative likelihood of the model. Deviance is a generalization of the variance. I fact, deviance should be meaured with respect to a staturated model (number of parmeters = number of observations), but this step is often skipped.\n",
    "\n",
    "The R `MASS` library contains the `stepAIC` function which uses the AIC to perform stepwise regression on the model. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(MASS)\n",
    "lm.step = stepAIC(lm.males, direction = 'both')\n",
    "lm.step$anova # ANOVA of the result \n",
    "summary(lm.step) # Summary of the best model\n",
    "plot(lm.step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary of `stepAIC` shows the steps taken. In the first step the model without `father.sqr` has the lowest AIC, so this feature is pruned. In the next step, the model eliminating `mother` has the lowest AIC, and so this feature is pruned. At the last step, the model which eleminates no features, denoted `<none>` has the best AIC. \n",
    "\n",
    "The three coeficients for the final model are now all significant. The residual plots show similar behavior to the intial model.\n",
    "\n",
    "### Adding an interaction term\n",
    "\n",
    "We will try one last idea, adding an interaction term. In this case we will compute all possbile interactions between the heights of the mother and the father, `mother`, `father`, `mother times father`. \n",
    "\n",
    "Execute the code in the cell below to compute the model and print and plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm.interaction = lm(childHeight ~ mother*father, data = males.ext)\n",
    "summary(lm.interaction)\n",
    "plot(lm.interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is clear from the summary that the interaction term is not significant. The best model we have to this point is the one computed with stepwise regression. \n",
    "\n",
    "***\n",
    "**Note:** Stepwise regression appears to be a simple method for feature selection. However, be aware that **stepwise regresson does not scale well**. As with any multiple comparison method, stepwise regresson suffers from a high probability of false positive results. In this case, a feature which should be dropped might not be, because of a low p-value or AIC. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization 1, Singular Value Decomosition\n",
    "\n",
    "Now that we have explored both manual feature selection and stepwise regression, we will examine regularization methods. Regularization methods stabalize the inverse of the **model matrix**. In this section we will use the singular value decomposition method to stabalize a model matrix. \n",
    "\n",
    "You may well wonder why we need regularization methods, when we have tools like stepwise regression. Two improtant reasonse are:\n",
    "\n",
    "- Stepwise regression is a compuationally intensive process, since we must recompute the model many times. There are methods that allow computation of the updated model, but with a large number of features there are numerious permutations. We need methods that can handel hundres, thousands, and even millions of features. \n",
    "- With stepwise regression a feature is either in or out of the model. This may not be the best choise. Perhaps a reweighting the features in some way might be better. \n",
    "- Stepwise regression suffers from issues inherent in multiple comparisons. \n",
    "\n",
    "### Linear Algebra Review\n",
    "\n",
    "Before we get into the details of regularizaton, let's review some basic linear algebra.\n",
    "\n",
    "**Note**, a simple reference guide to common linear algebra operations in R can be found at on the [Quick R website](http://www.statmethods.net/advstats/matrix.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating two vectors of length 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = rep(2, 3)\n",
    "a\n",
    "b = 1:3\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform some basic element-wise arithmetic opertions on vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a + b\n",
    "a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute the **dot product**, **scalar product** or **inner product** of two vectors of equal lenght.\n",
    "\n",
    "$$dot\\ product = \\Sigma_i^n a_i \\cdot b_i$$\n",
    "\n",
    "Give this a try by executing the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t(a) %*% b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The square root of the inner product of a vector with itself is the length or $L2$ norm of the vector.\n",
    "\n",
    "$$\\parallel a \\parallel = length\\ of\\ vector\\ a = \\sqrt{a \\cdot a}$$\n",
    "\n",
    "**Your Turn:** Create and exectue the code to compute the length or norm of the vector `a` in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write the inner product as:\n",
    "\n",
    "$$a \\cdot b = \\parallel a \\parallel \\parallel b \\parallel cos(\\theta)\\\\\n",
    "or \\\\\n",
    "cos(\\theta) = \\frac{a \\cdot b}{\\parallel a \\parallel \\parallel b \\parallel}$$\n",
    "\n",
    "Notice that the inner product of orthogonal vectors is $0$. Run the code in the cell below to see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa = c(1, 0, 0)\n",
    "bb = c(0, 1, 1)\n",
    "t(aa) %*% bb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try some operations on martricies. Let $A$  and $B$ be $m = 4$ rows by $n = 3$ columns matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = matrix(4, nrow = 4, ncol = 3)\n",
    "A\n",
    "B = matrix(1:12, nrow = 4, ncol = 3)\n",
    "B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform some arithmatic operations element by element on these matrces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A + B\n",
    "A * B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can multiply a $mxn$ matrix by a vector of length $n$ by taking the inner product of each row of the matrix and the vector. The result in a vector of length $n$. Each element of the result can be written at:\n",
    "\n",
    "$$y_i = \\Sigma_j^m A_{ij} \\cdot b_j$$\n",
    "\n",
    "Run the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A %*% b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, how do we multipy two matrices? In matrix multiplcation each element of the resulting matrix is the inner product of a row by a column. For example, the element $Y_{ij}$ of the result matrix is computed as follows:\n",
    "\n",
    "$$Y_{ij} = \\Sigma_j^m A_{ij} \\cdot B_{ji}$$\n",
    "\n",
    "Notice that the number of columns, $m$, of the first matrix must equal the number of rows of second matrix. And, that the number of rows, $n$ of the first matrix must equal the number of columns of the second metrix. In this case, the two matricies are said to be **conformable**. \n",
    "\n",
    "Give matrix multiplication a try by exectuing the code in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A %*% B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That operation failed! Evidently these matrices are not conformable. \n",
    "\n",
    "But, what if we take the transpose of $B$? The **transpose** of a matrix is just that matrix with the row and column indicies permuted like this:\n",
    "\n",
    "$$B_{ji}^T = B_{ij}\\\\\n",
    "where \\\\\n",
    "B\\ has\\ dimensions\\ n x m \\\\\n",
    "and \\\\\n",
    "B^T\\ has\\ dimensions\\ m x n$$ \n",
    "\n",
    "If we multiply an $n x m$ matix by an $m x n$ matrix the result is a square $n x n$ matrix. \n",
    "\n",
    "**Your Turn:** create and execute the code in the cell below to multiply the matrix A by the transpose of B. Use the R `t()` function to take the transpose of the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the **indentity** matrix having ones on the diagonal and zeros elsewehere.\n",
    "\n",
    "$$I = \\begin{bmatrix}\n",
    "    1  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & 1 & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The identity multiplied by any matrix gives that matrix. If $AB$ is a rectangular matrix then:\n",
    "\n",
    "$$AB = I \\cdot AB = AB \\cdot I$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "I = diag(c(1, 1, 1, 1))\n",
    "I\n",
    "AB %*% I\n",
    "I %*% AB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle we can compute an inverse of a matrix so that:\n",
    "\n",
    "$$A = A\\\\\n",
    "A = AI \\\\\n",
    "A^{-1}A = I$$\n",
    "\n",
    "In machine learning, we often encounter matricies where cannot be inverted directrly. Instead, we need a decompositon of $A$ that alows us to compute $A^{-1}$. One possibility is a method called singular value decomposiion or SVD:\n",
    "\n",
    "$$svd(A) = U D V^{\\ast}$$\n",
    "\n",
    "Where,\n",
    "- $U$ are the orthogonal unit norm left singular vectors.\n",
    "- $V$ are the orthogonal unit norm right singular vectors, and $V^{\\ast}$ is the conjugate transpose. For real-valued $A$ this is just $V^T$.\n",
    "- $D$ is a diagonal matrix of singular values, which are said to define a **spectrum**.\n",
    "- $A$ is comprised of the linear combination of singular vectors scaled by singular values.\n",
    "\n",
    "To compute the SVD of a matrix and view the results execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SVD = svd(B)\n",
    "SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify that the singular vectors form a orthonomal basis by executing the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t(SVD$u) %*% SVD$u\n",
    "SVD$v %*% t(SVD$v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result are two identity matricies, one of dimension $n x n$ and the other of dimenstion $m x m$.\n",
    "\n",
    "We can view the product of the matrix $A$ with a vector as defining a rotation and scaling. The singular value decomposition of $A$ can be viewe as:\n",
    "\n",
    "- A first rotation defined by the unit norm singular values $V^{\\ast}$.\n",
    "- A scaling defined by the diagonal singular value matrix $D$.\n",
    "- A second rotation defined by the unit norm singular values $U$.\n",
    "\n",
    "This geometric interpertation can be visualized as shown in the figure below.\n",
    "\n",
    "![](img/SVD.png)\n",
    "\n",
    "Execute the code in the cell below and examine the rotations and scaling of the inital vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e = sqrt(1/3)\n",
    "u = c(e, e, e)\n",
    "u\n",
    "sqrt(u %*% u)\n",
    "cat('The first rotation')\n",
    "u = t(SVD$v) %*% u\n",
    "u\n",
    "sqrt(t(u) %*% u)\n",
    "cat('The scaling')\n",
    "u = diag(SVD$d) %*% u\n",
    "u\n",
    "sqrt(t(u) %*% u)\n",
    "cat('The second rotation')\n",
    "u = SVD$u %*% u\n",
    "u\n",
    "sqrt(t(u) %*% u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Do the following:\n",
    "- Demonstrate that the matrix `B` gives the same rotation and scale by multipling a vector `u = c(e, e, e)`. \n",
    "- Verify that the singular value decomposition is equal to the original matrix. Compute $U D V^T$. Make sure you use the full diagonal matrix of the singular values using the R code, `diag(SVD$d)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question now is, how do we compute $A^{-1}$ from its SVD? In most cases, there is no exact representation. But, we can come close using the **pseudo inverse** also known as the **Moore-Penrose inverse**, which is defined as:\n",
    "\n",
    "$$A^\\dagger = V D^+ U^*$$\n",
    "\n",
    "Where,\n",
    "\n",
    "- $D^+$ is the square diagonal matrix of **inverse sigular values** significantly greater than $0$. All other terms are set to $0$.\n",
    "- $U^*$ is the transpose of the right sigular value matrix. \n",
    "- $V$ is the left singular value matrix.\n",
    "\n",
    "The matrix, $A$ may not be of full rank. The types of long and narrow $n x m$ matricies we encounter in machine learning are typically **rank deficient**. A rank deficient matrix arrises when there is linear dependency between one or more of the columns. As an example, a matrix with correlated (not necessiarily perfectly correlated) columns is bound to be rank deficient. \n",
    "\n",
    "A matrix is considered rank deficient if it has one or more of the $m$ sigular values  $d_i  \\sim 0.0$. In this case we substitute $0.0$ values on the diagonal of $D^+$ where the singular values $d_i \\sim 0.0$. In fact, we want $d_i$ to be significanlty greater than $0$. \n",
    "\n",
    "Let's try an example. The code in the cell below computes the SVD of a matrix of random numbers chosen from a Normal distribution. The pseudo inverse is computed and multiplied by the original matrix. Execute this code and note the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat('Create a matrix of random values')\n",
    "C = matrix(rnorm(9), nrow = 3, ncol = 3)\n",
    "C\n",
    "cat('Compute the SVD and look at the sigular values')\n",
    "CSVD = svd(C)\n",
    "CSVD$d\n",
    "cat('The inverse matrix of sigular values')\n",
    "D = diag(1/CSVD$d)\n",
    "D\n",
    "cat('The pseudo inverse of the matrix')\n",
    "cInv = CSVD$v %*% D %*% t(CSVD$u)\n",
    "cInv\n",
    "cat('The pseudo inverse times the matrix')\n",
    "cInv %*% C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, notice that the singular values are of similar magnitude and none are near zero. This matrix is not rank deficient. \n",
    "\n",
    "Let's try another example. The code in the cell below does the following:\n",
    "\n",
    "- Creates a $4 X 4$ matrix of numbers drawn from a Normal distribution.\n",
    "- Substitues values in the 4th column which are a linear combination of the other three columns.\n",
    "- Computes the SVD of this matrix.\n",
    "- Creates the inverse diagonal matrix of singular values.\n",
    "- Computes the pseudo inverse of the matrix.\n",
    "- Multiplies the pseudo inverse by the original matrix. \n",
    "\n",
    "Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat('Create a matrix of random values')\n",
    "C = matrix(rnorm(16), nrow = 4, ncol = 4)\n",
    "C[, 4] = 0.4 * C[, 1] + 0.2 * C[, 2] + 0.4 * C[, 3]\n",
    "C\n",
    "cat('Compute the SVD and look at the sigular values')\n",
    "CSVD = svd(C)\n",
    "CSVD$d\n",
    "cat('The inverse matrix of sigular values')\n",
    "D = diag(1/CSVD$d)\n",
    "D\n",
    "cat('The pseudo inverse of the matrix')\n",
    "cInv = CSVD$v %*% D %*% t(CSVD$u)\n",
    "cInv\n",
    "cat('The pseudo inverse times the matrix')\n",
    "cInv %*% C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about this result:\n",
    "\n",
    "- The 4th singular value is nearly zero. Evidently, this matrix is rank deficient. \n",
    "- The product of the pseudo inverse is not close to being the identity matrix. This is the result of taking using the unstable inverse of the rank deficient matrix.\n",
    "\n",
    "**Your Turn:** In the cell below do the following:\n",
    "\n",
    "- Set the inverse of the smallest singular value to zero.\n",
    "- Compute the pseudo inverse.\n",
    "- Compute the product of the pseudo inverse and the orginal matrix. \n",
    "- Note if the result is closer to an identity matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression with the Pseudo Inverse\n",
    "\n",
    "We have already looked at feature selection using manual trial and error methods and stepwise regression. How can we use the pseudo inverse to create a regularized regression? \n",
    "\n",
    "Let's start by examining the linear regresson problem. The goal is to compute a vector of **model coefficients** or weights which minimize the mean squared residuals, given a vector of data $x$ amd a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "However, this is hard to do directly in practice.\n",
    "\n",
    "- In typical case $A$ is long and narrow. In other words we have more data **cases** than coeficients. \n",
    "- Solving for $A^{-1}$ is computationally difficult and inefficient.\n",
    "- Solution is numerically unstable if $A$ is rank deficient. \n",
    "\n",
    "One way to deal with the problem of rank deficiency is to use the pseudo inverse $A^\\dagger$. Recalling the singular value decomposition of $A$ we can write:\n",
    "\n",
    "$$A^\\dagger = V D^+ U^*$$\n",
    "\n",
    "***\n",
    "**Note:** In practice, the direct compuation of a pseudo inverse is rarely used for linear models. Instead, more compuationally efficient methods such as QR decomposition are often used. Discussion of these methods is beyond the scope of this course. Details can be found in many sources including the seminal book titled [Matrix Computations](http://web.mit.edu/ehliu/Public/sclark/Golub%20G.H.,%20Van%20Loan%20C.F.-%20Matrix%20Computations.pdf) by Gene Golub and Charles van Loan.\n",
    "***\n",
    "\n",
    "As a first step we need to create a model matrix of the features for the Gaulton height data. Execute the code in the cell below which creates a matrix from the features in a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M = as.matrix(males.ext[, c('mother', 'father', 'mother.sqr', 'father.sqr')])\n",
    "head(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Your Turn:** The next step is to compute the SVD of the resulting matrix and examine the singular values. In the cell below create and exectue code to do the following:\n",
    "1. Use the `svd` function to compute the SVD of the matrix `M`, and call your result `svdM`. \n",
    "2. Print the following: \n",
    "  - The singular values, `d`.\n",
    "  - The first 10 rows of the left singular vectors `u`.\n",
    "  - The right singular vectors `v`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the 3rd and 4th singular values are serveral orders of magnitude smaller than the first two. This matrix is most likelily rank deficient. \n",
    "\n",
    "Now that you have computed and examinded the SVD of the model matrix you will now compute the **pseudo inverse** using the first two singular values. The code in the cell below computes the pseudo inverse, pringing some intermediate results. As a check, the product of  the pseudo inverse and the model matrix is computed and printed. Execute this code and  examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cat('Compute and print the inverse singular value matrix')\n",
    "d.trim = rep(0, 4)\n",
    "d.trim[1:2] =1/ svdM$d[1:2]\n",
    "dM = diag(d.trim)\n",
    "dM\n",
    "cat('Compute and print the pseudo inverse')\n",
    "invM = svdM$v %*% dM %*% t(svdM$u)\n",
    "invM\n",
    "cat('Compute and print the matrix invMM')\n",
    "invMM = invM %*% M\n",
    "invMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The product of the pseudo inverse is a symmetric matrix. The structure is a bit odd. The diagonal elements are all about $0.5$ and the off-diagonal elements aree either approximately  $0.0$ or $0.5$.\n",
    "\n",
    "***\n",
    "**Your Turn:** Now that we have a pseudo inverse of the model matrix we can compute the model coeficients. The model coeficients are just the matrix product of the pseudo inverse with the observations, or heights of the adult male childern in this case. In the cell below write and execute the code to compute the model coeficient vector, named `b`. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a vector of model coefficients, its time to evaluate our model.  We use the following steps:\n",
    "\n",
    "- Compute the predicted values or scores using the product of the model matrix and the model coefficients. We need to add the mean of the label values.\n",
    "- Compute the residuals.\n",
    "- Display residual plots and summary statistics.\n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "males.ext$score = M %*% bM + mean(males.ext$childHeight)\n",
    "males.ext$resids = males.ext$score - males.ext$childHeight\n",
    "\n",
    "require(repr)\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "\n",
    "plot.svd.reg <- function(df, k = 4){\n",
    "  require(ggplot2)\n",
    "  require(gridExtra)\n",
    "  \n",
    "  p1 <- ggplot(df) + \n",
    "            geom_point(aes(score, resids), size = 2) + \n",
    "            stat_smooth(aes(score, resids)) +\n",
    "            ggtitle('Residuals vs. fitted values')\n",
    " \n",
    "  p2 <- ggplot(df, aes(resids)) +\n",
    "           geom_histogram(aes(y = ..density..)) +\n",
    "           geom_density(color = 'red', fill = 'red', alpha = 0.2) +\n",
    "           ggtitle('Histogram of residuals')\n",
    "\n",
    "  qqnorm(df$resids)\n",
    "    \n",
    "  grid.arrange(p1, p2, ncol = 2)\n",
    "    \n",
    "  df$std.resids = sqrt((df$resids - mean(df$resids))^2)  \n",
    "    \n",
    "  p3 = ggplot(df) + \n",
    "            geom_point(aes(score, std.resids), size = 2) + \n",
    "            stat_smooth(aes(score, std.resids)) +\n",
    "            ggtitle('Standardized residuals vs. fitted values')\n",
    "  print(p3) \n",
    "    \n",
    "  n = nrow(df)\n",
    "  Ybar = mean(df$childHeight)\n",
    "  SST <- sum((df$childHeight - Ybar)^2)\n",
    "  SSR <- sum(df$resids * df$resids)\n",
    "  SSE = SST - SSR\n",
    "  cat(paste('SSE =', as.character(SSE), '\\n'))\n",
    "  cat(paste('SSR =', as.character(SSR), '\\n'))\n",
    "  cat(paste('SST =', as.character(SSE + SSR), '\\n'))\n",
    "  cat(paste('RMSE =', as.character(SSE/(n - 2)), '\\n'))\n",
    "\n",
    "  adjR2  <- 1.0 - (SSR/SST) * ((n - 1)/(n - k - 1))\n",
    "  cat(paste('Adjusted R^2 =', as.character(adjR2)), '\\n')\n",
    "}\n",
    "\n",
    "plot.svd.reg(males.ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are comparable to the results we obtained by stepwise regression. Evidently, the pseudo inverse method worked rather well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularization 2, Ridge and Lasso Regression\n",
    "\n",
    "So far, we have looked at two approached for dealing with over-parameterized models; feature selection by stepwise regresson and singular value decomposision. In this sectio we will explore the most widely used regularization method for optimization-based machine learning models, ridge regression. \n",
    "\n",
    "Let's start by examining the **normal equation** formulation of the linear regresson problem. The goal is to compute a vector of **model coefficients** or weights which minimize the mean squared residuals, given a vector of data $x$ amd a **model matrix** $A$. We can write our model as:\n",
    "\n",
    "$$x = A b$$\n",
    "\n",
    "To solve this problem we would ideally like to compute:\n",
    "\n",
    "$$b = A^{-1}x$$\n",
    "\n",
    "The commonly used Normal Equation form can help:\n",
    "\n",
    "$$b = (A^TA)^{-1}A^Tx$$\n",
    "\n",
    "Now, $A^TA$ is an $m x m$ matrix, and thus is of reduced dimension. But, **$A^TA$ can still be rank deficient!** \n",
    "\n",
    "The basic idea of ridge regression is to stabalize the inverse sigular value matrix,$D^+$, by **adding a small bias term**, $\\lambda$, to each of the singular values. We can state this operation in matrix notation as follows. We start with a modified form of the normal equations (also know as the **L2 or Euclidean norm** minimization problem):\n",
    "\n",
    "$$min [\\parallel A \\cdot x - b \\parallel + \\parallel \\lambda \\cdot b\\parallel]\\\\  or \\\\\n",
    "b = (A^TA + \\lambda^2)^{-1}A^Tx$$\n",
    "\n",
    "In this way, the values of small signular values do not blow up when we compute the inverse. You can see this by writing out the $D^+$ matrix with the bias term.\n",
    "\n",
    "$$D_{ridge}^+  = \\begin{bmatrix}\n",
    "    \\frac{1}{d_1 + \\lambda^2}  & 0 & 0 & \\dots & 0 \\\\\n",
    "    0  & \\frac{1}{d_2 + \\lambda^2} & 0 & \\dots & 0 \\\\\n",
    "    \\vdots &\\vdots &\\vdots & & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\dots & \\frac{1}{d_m + \\lambda^2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Adding this bias term creates a 'ridge' in the singular value matrix, giving this method its name **ridge regression**. \n",
    "\n",
    "You can also think of ridge regression as limiting the L2 or Euclidean norm of the values of the model coeficient vector. The value of $\\lambda$ determines how much the norm of the coeficient vector constrains the solution. You can see a view of this geometric interpretaton in the figure below.  \n",
    "\n",
    "![](img/L2.jpg)\n",
    "<center> **Geometric view of L2 regularization**\n",
    "\n",
    "The same method goes by some other names, as it seems to have been 'invented' several times. In particular, **Tikhonov regularization**, or **L2 norm regularization**. In all likelihood the method was first developted by the Russian mathematician Andrey Tikhonov in the late 1940's, and first published in english in 1977.\n",
    "\n",
    "![](img/Tikhonov_board.jpg)\n",
    "<center> **Commemorative plaque for Andrey Nikolayevich Tikhonov at Moscow State University**\n",
    "\n",
    "Let's give this a try. Execute the code in the cell below which computes the $(A^TA + \\lambda^2)^{-1}A^T$ matrix with a lambda value of `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat('Compute and print the inverse singular value matrix')\n",
    "lambda = 100.0\n",
    "d.trim = 1/ (svdM$d + lambda)\n",
    "mD = diag(d.trim)\n",
    "mD\n",
    "cat('Compute and print the pseudo inverse')\n",
    "mInv = svdM$v %*% mD %*% t(svdM$u)\n",
    "mInv\n",
    "MTMTM = mInv %*% (M)\n",
    "MTMTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn!:** Compute the model coeficients using the $(A^TA + \\lambda^2)^{-1}A^T$ matrix you just computed. Give the result a new name and compare these results to the previously computed model coeficients. Call the new vector of model coeficients `b2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to compute the score and residuals, along with the model summary. Execute this the code below and compare these results to the one perviously computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "males.ext$score = M %*% b2 + mean(males.ext$childHeight)\n",
    "males.ext$resids = males.ext$score - males.ext$childHeight\n",
    "\n",
    "plot.svd.reg(males.ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results are quite similar to those computed using the SVD truncation method. Both results provide resonable regularizaitons. \n",
    "\n",
    "**Your Turn:** Create a new ridge regression model using $\\lambda = 1.0$. Compare the new model to the models computed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that for small bias terms the solution does not change very much. However, the numeric range of the inverse singular values is much greater. In other words, you are applying much less of a regularization or bias. \n",
    "\n",
    "### Bias-variance trade-off\n",
    "\n",
    "The `glmnet` package, by Hastie and Quinn, allows us to compute a sequence of ridge regression solutions. The code in the cell below computes solutions for `20` values of $\\lambda$. Since we are performing linear regression we use a Gaussian model family for the error terms. Execute this code and examine the values of the model coefficients as $\\lambda$ increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "require(glmnet)\n",
    "b = as.matrix(males.ext$childHeight)\n",
    "mod.ridge = glmnet(M, b, family = 'gaussian', nlambda = 20, alpha = 0.0)\n",
    "plot(mod.ridge, xvar = 'lambda', label = TRUE)\n",
    "plot(mod.ridge, xvar = 'dev', label = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that $\\lambda$ increases, the values of the 4 model coeficients decrease toward zero. When all coeficients are zero, the model predicts all values of the label as zero! In other words, high values of $\\lambda$ give highly biased soluions, but with very low variance. For small values of $\\lambda$, the situation is just the oposite. The solution has low bias, but is quite unstable, having maximum variance. This **bias-variance trade off** is a key concept in machine learning. \n",
    "\n",
    "### Lasso regression\n",
    "\n",
    "We can also do regularization using other norms. **Lasso or L1 regularizaton** limits the sum of the absolute values of the model coefients. The L1 norm is sometime know as the **Manhattan norm**, since distrance are measured as if you were traveling on a rectangular grid of streets. \n",
    "\n",
    "You can also think of lasso regression as limiting the L1 norm of the values of the model coeficient vector. The value of $\\lambda$ determines how much the norm of the coeficient vector constrains the solution. You can see a view of this geometric interpretaton in the figure below.  \n",
    "\n",
    "![](img/L1.jpg)\n",
    "<center> **Geometric view of L1 regularization**\n",
    "\n",
    "By setting the `alpha` argument to `glmnet` to zero you can perform lasso regresson. Execute the code in the cell below to compute and evaluate a lasso regression model with 20 values of lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod.lasso = glmnet(M, b, family = 'gaussian', nlambda = 20, alpha = 1.0)\n",
    "plot(mod.lasso, xvar = 'lambda', label = TRUE)\n",
    "plot(mod.lasso, xvar = 'dev', label = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that model coeficients are much more tightly constrianed than for L2 regularization. In fact, only two of the possible model coeficients have non-zero values at all. This is typical of L1 or lasso regression.\n",
    "\n",
    "### Elastic net regression\n",
    "\n",
    "The **elastic net** algorthm uses a weighted combination of L2 and L1 regularization. The `alpha` argument to the `glmnet` function determines the relative weight of the two methods. If `alpha = 0`, the regularization is pure L2 and if `alpha = 1.0` the regularization is pure L1. \n",
    "\n",
    "The code in the cell below gives equal weight to each regression method. Excute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod.ridge.lasso = glmnet(M, b, family = 'gaussian', nlambda = 20, alpha = 0.5)\n",
    "plot(mod.ridge.lasso, xvar = 'lambda', label = TRUE)\n",
    "plot(mod.ridge.lasso, xvar = 'dev', label = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the elastic net model combines some of the behaviors of both L2 and L1 regularization. \n",
    "\n",
    "As a next step, we will evaluate this model by the following steps.\n",
    "- Create a score vector using the `predict` method on the model and using the original model matrix.\n",
    "- Compute residuals.\n",
    "- Plot and print summaries of the model performance.\n",
    "\n",
    "Run this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "males.ext$score = predict(mod.ridge.lasso, newx = M)[, 15]\n",
    "males.ext$resids = males.ext$score - males.ext$childHeight\n",
    "\n",
    "plot.svd.reg(males.ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar to those obtained using both stepwise regression and the pseudo inverse method. For this small problem all three methods seem to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables and the Model Matrix\n",
    "\n",
    "Up until now we have only been working with numeric data. How can we handle categorical variables in numeric models? \n",
    "\n",
    "We need to encode the categorical variables into one or more numeric variables. The common approach is to convert the categorical variable to a set of binary **dummy variables** or **indicator variables**. \n",
    "\n",
    "In R, and some other analytical software, this is done automatically. In R we can use the `model.matrix` function to compute and view the result explicitly. \n",
    "\n",
    "The code in the cell below computes the scaled model data frame. Execute this code and examine the summary of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Galton.scaled = GaltonFamilies[, c('mother', 'father', 'childHeight', 'gender')]\n",
    "Galton.scaled = mutate(Galton.scaled, \n",
    "                       mother.sqr = mother^2, father.sqr = father^2)\n",
    "Galton.scaled[, c('mother', 'father', 'mother.sqr', 'father.sqr')] = \n",
    "        lapply(Galton.scaled[, c('mother', 'father', 'mother.sqr', 'father.sqr')], \n",
    "               scale)\n",
    "str(Galton.scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical (factor) variables are converted to one or more dummy variables. \n",
    "\n",
    "Execute the  code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod.mat = model.matrix(childHeight ~ ., data = Galton.scaled)\n",
    "mod.mat[1:10, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "typeof(mod.mat[, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the categorical variable `gender` is now coded as a single numeric variable with two possible values.\n",
    "\n",
    "**Your Turn:** Create and execute the code to print the first few lines of a the model matrix without an intercept term. **Hint**, you need to add `-1` to the model formula to drop the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the coding of the `gender` feature has changed. Why do you think this is?\n",
    "\n",
    "Now, it is time to compute and evaluate the model. The code in the cell below does the following:\n",
    "- Compute the model.\n",
    "- Compute the scores from the model using the original model matrix.\n",
    "- Compute the residuals.\n",
    "- Plot and print model performance diagnositics. \n",
    "\n",
    "Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.mat = as.matrix(Galton.scaled$childHeight)\n",
    "mod.ridge.gender = glmnet(mod.mat, b.mat, family = 'gaussian', \n",
    "                          nlambda = 20, alpha = 0.5)\n",
    "\n",
    "Galton.scaled$score = predict(mod.ridge.gender, newx = mod.mat)[, 15]\n",
    "Galton.scaled$resids = Galton.scaled$score - Galton.scaled$childHeight\n",
    "\n",
    "plot.svd.reg(Galton.scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that this model has both a larger $R^2$ and RMSE. Why do you think this is?\n",
    "\n",
    "**Your Turn:** For the model you have just created plot the model coeficients vs. $\\lambda$ and the deviance vs. $\\lambda$. How can you interpret these result with particular attention on the categorical feature, 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresson\n",
    "\n",
    "Untill now, we have been working strictly with linear regression models. Now we will look at a widely used variation on the linear model know as **logistic regresson**.\n",
    "\n",
    "Logistic regression is widely used as a classification model. Logistic regression is linear model, with a binary response, `{False, True}` or `{0, 1}`.  However, the response is computed as a log likelihood. In the simplest case, the response has a Binomial distribution. \n",
    "\n",
    "The response of the linear model is transformed to the log likelihood using a sigmodial function, also know as the **logistic function**:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-\\kappa(x - x_0)}} \\\\\n",
    "\\kappa = steepness$$\n",
    "\n",
    "Execute the code in the cell below to compute and plot an example of the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(repr.pmales.extlot.width=6, repr.plot.height=4)\n",
    "\n",
    "xseq = seq(-7, 7, length.out = 500)\n",
    "plot.logistic = function(v){\n",
    "  require(ggplot2)\n",
    "  logistic = exp(xseq - v)/(1 + exp(xseq - v))\n",
    "  df = data.frame(x = xseq, y = logistic)\n",
    "  ggplot(df, aes(x,y)) +\n",
    "    geom_line(size = 2, color = 'red') +\n",
    "    geom_vline(xintercept = v, size = 2, color = 'black') +\n",
    "    geom_hline(yintercept = 0.5, size = 2, color = 'black') +\n",
    "    ylab('log likelihood') + xlab('Value of x') +\n",
    "    ggtitle('Logistic function for two-class classification') +\n",
    "    theme_grey(base_size = 18)\n",
    "}\n",
    "plot.logistic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this a bit more concrete with a simple example. Say we have a linear model:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1\\ x$$\n",
    "\n",
    "Now, depending on the value of $\\hat{y}$ we want to classify the output from a logistic regression model as either `0` or `1`. We can use the linear model in the logistic function as follows:\n",
    "\n",
    "$$F(\\hat{y}) = \\frac{1}{1 + e^{-\\kappa(\\beta_0 + \\beta_1\\ x)}} $$\n",
    "\n",
    "In this way we transform the continious output of the linear model defined on $-\\infty \\le \\hat{y} \\le \\infty$ to a binary response, $0 \\le F(\\hat{y}) \\le 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clasification Example\n",
    "\n",
    "Next, we will try to classify the gender of the childern in the Gaulton families data set using logistic regression on the height data.\n",
    "\n",
    "As a first step, we need to create a scaled model matrix of the features for the logistic regression. Run the code in the cell bellow to compute this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Galton.gender = GaltonFamilies[, c('mother', 'father', 'childHeight', 'gender')]\n",
    "Galton.gender[, c('mother', 'father', 'childHeight')] = \n",
    "        lapply(Galton.gender[, c('mother', 'father', 'childHeight')], \n",
    "               scale)\n",
    "mod.gender = model.matrix(gender ~ ., data = Galton.gender)\n",
    "mod.gender[1:10, ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below using the `glmnet` function to compute the regression of to predict gender using a binomial distribution. The score or response is then computed and printed. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require(glmnet)\n",
    "b.gender = as.matrix(Galton.gender$gender)\n",
    "b.gender = ifelse(b.gender == 'male', 1, 0)\n",
    "mod.class = glmnet(mod.gender, b.gender, family = 'binomial', \n",
    "                          nlambda = 20, alpha = 0.5)\n",
    "\n",
    "Galton.gender$score = predict(mod.class, newx = mod.gender)[, 15]\n",
    "Galton.gender[1:10, c('score', 'gender')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to transfom the response to the binary output using the logistic function. The code in the cell below applies the logistic transformation and sets the categories for the score. Run this code and  examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Galton.gender$score = exp(Galton.gender$score)/(1 + exp(Galton.gender$score))\n",
    "Galton.gender$score = ifelse(Galton.gender$score > 0.5, \"male\", \"female\")\n",
    "Galton.gender[1:10, c('score', 'gender')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes some common performance statistics for binary classifiers. The overall performance of the binary classifier is summarized by the elements of a **confusion matrix**. \n",
    "\n",
    "Run this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logistic.eval <- function(df){ \n",
    "  # First step is to find the TP, FP, TN, FN cases\n",
    "  df$conf <- ifelse(df$gender == 'male' & df$score == 'male', 'TP',\n",
    "                    ifelse(df$gender == 'female' & df$score == 'male', 'FP',\n",
    "                           ifelse(df$gender == 'female' & df$score == 'female', 'TN', 'FN')))\n",
    "\n",
    "  # Elements of the confusion matrix\n",
    "  TP = length(df[df$conf == 'TP', 'conf'])\n",
    "  FP = length(df[df$conf == 'FP', 'conf'])\n",
    "  TN = length(df[df$conf == 'TN', 'conf'])\n",
    "  FN = length(df[df$conf == 'FN', 'conf'])\n",
    "    \n",
    "  out <- data.frame(Positive = c(TP, FP), Negative = c(FN, TN))\n",
    "  row.names(out) <- c('TruePos', 'TrueNeg')\n",
    "  print(out)  \n",
    "  \n",
    "  # Compute and rint metrics\n",
    "  cat('\\n')\n",
    "  cat(paste('accuracy =', as.character((TP + TN)/(TP + TN + FP + FN)), '\\n'))      \n",
    "  cat(paste('precision =', as.character(signif(TP/(TP + FP)), digits = 2)), '\\n')     \n",
    "  cat(paste('recall =', as.character(TP/(TP + FN))))\n",
    "}\n",
    "logistic.eval(Galton.gender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the confusion matrix males are defined as Positive cases and females are Negtive cases. Notice that most of the cases in this data are correctly classified with only a few false negatives and false positives.\n",
    "\n",
    "The other metrics are defined as follows:\n",
    "\n",
    "- Accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "- Precision = TP / (TP + FP)\n",
    "- Recall = TP / (TP + FN)\n",
    "\n",
    "These summary statistics show the classifier works fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2017, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
