{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analytics\n",
    "\n",
    "### Data Science 350\n",
    "### Stephen Elston\n",
    "\n",
    "## Introduction \n",
    "\n",
    "This notebook contains a tutorial introduction to basic text analytics with R.  \n",
    "\n",
    "![](img/HAL.jpg)\n",
    "<center>**AI has a ways to go!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "***\n",
    "**Note** To run this notebook you will need to install the following R packages:\n",
    "\n",
    "- tm\n",
    "- slam\n",
    "- topicmodels\n",
    "- SnowballC\n",
    "- RTextTools\n",
    "- ggplot2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Are Everywhere\n",
    "\n",
    "Raw text data is an unstructured and ubiquitious type of data. Most of the world’s data is unstructured. Volumes of unstructured data, including text, are growing much faster than structured data. There are many industry estmates for the fraction of all data which is unstructured. A few from the last 8 years include:   \n",
    "- 2009 HP Survey: 70%\n",
    "- Gartner: 80%\n",
    "- Teradata: 85%\n",
    "- But, **Beware of industry estimates!!**\n",
    "\n",
    "How much text data are we talking about here? In a few years time, Twitter has more text data recorded than all that has been written in print in the history of mankind. (http://www.internetlivestats.com/twitter-statistics/)\n",
    "\n",
    "### Applications of Text Analytics\n",
    "\n",
    "Given the ubiquity and volume of text data, it is not surprising that numerious powerful applications which exploit text analytics are appearing. A few of these applications are listed below.\n",
    "\n",
    "- Intelligent applications\n",
    "  - Assistants\n",
    "  - Chat bots\n",
    "- Classification\n",
    "  - Sentiment analysis\n",
    "  - SPAM detection\n",
    "- Speech recognition\n",
    "- Search\n",
    "- Information retrieval\n",
    "- Legal discovery\n",
    "\n",
    "### Analysis of Text Data\n",
    "\n",
    "In this tutorial we investigate three areas of text analytics. The following three sections cover these topics.\n",
    "\n",
    "- Preparing text for analysis.\n",
    "- Classification of text and sentiment analysis.\n",
    "- Topic Models for document classification and retrival. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing  text data\n",
    "\n",
    "By its very nature, text data comes unstructured and poorly organized for analysis. Typically multiple steps are required to process text into a form suitable for analysis. You can think of this process as transforming the unstructured data into a structured set of features. \n",
    "\n",
    "Steps covered in this tutorial include the following:\n",
    "\n",
    "- Organize text documents into a corpus\n",
    "- Normalize the text to remove unneeded content\n",
    "  - Tokenize text\n",
    "  - Clean text\n",
    "- Create term document matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text analysis methods\n",
    "\n",
    "There are a great many approaches which have been tried for text analytics and natural language processing (NLP). We only mention a few below. \n",
    "\n",
    "- The **bag of words model** is a simple widely used and suprsingly effective model for analysis of text data. The BOW model uses only on the frequency of the words in the document and order of the words is not consisdered. Dispite these seamingly rediculous assumption, the model works well in many cases. \n",
    "  - The BOW model assumes **exchangeability** of words. \n",
    "  - The end product of applying the BOW model is a term-document or document-term matrix. The tdm, or dtm is a structured representation of word frequency by document. \n",
    "  - The tdm or dtm can be used for classification if lables are available or clustering for unspervised learning. \n",
    "- Other powerful models are the **word to vec** and **doc to vec** models. Word to vec, uses a neural network model to determine similarity between words. These models are beyond the scope of this tutorial. You can find a good introduction to this model in the [article by Rong](https://arxiv.org/pdf/1411.2738.pdf)\n",
    "- Another widely used model is of **Part of Speech (PoS) Tagging**. PoS tagging attempts to label or anotate words in a corpus (e.g. a collection of documents) as, say nouns, verbs, pronouns, etc. PoS tagging is beyond the  scope of this tutorial. The PoS tagger creates a tree of the relationship of words in say a sentance. One useful specialization of PoS tagging is named entity recognition, which attempts to find proper nouns. \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####  Document Classification, Topic Models, and Information Retervial\n",
    "\n",
    "A common objective of text analysis is to classify and group documents. These methods have applicaiton in information retrival and search. Understandably, there are a great many such methods which have been developed over the years. We will only discuss a few examples in this tutorial.    \n",
    "\n",
    "- **Classification** is a widely used supervised learning method for docuent analysis. For example, documents can be classified as SPAM  or not SPAM or as positive or negative sentiment. \n",
    "- **Latent Sematic Analysis (LSA)** and **Doc-to-Vec** analysis are unsupervised learning methods used to determine which documents are closely related. These methods use similarity measures to rank doccuments as being related. These powerful methods are beyond the scope of this tutorial. \n",
    "- **Topic models** are an method wherein the a set of documents is categorized by one or more **topics**. analysis are unsupervised learning methods used to determine which documents are closely related. These methods use similarity measures to rank doccuments as being related.  \n",
    "  - **Latent Dirichlet Allocation (LDA)** allocates the probability that a document contains a topic.\n",
    "  - **Latent Sematic Analysis (LSA)** and **Doc-to-Vec**  These powerful methods are beyond the scope of this tutorial. \n",
    "- A variety of **distance metrics** have been developed to determine the distance between words, sentances and documents. These methods are related to coding theory widely used in telecommunications engineering.\n",
    "- **Clustering methods** are unsupervised learning models which seek to group similar documents into clusters. A variety of distance metrics can be used to define the structure of text clusters. \n",
    "  - K-means \n",
    "  - Hieratical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preparation\n",
    "\n",
    "Unstructured text must be processed into a uniform set of features suitable for further analysis. In this section we will step through some of the commonly used methods for convering unstructured text into a form we can use for analysis. There are three steps we need to transform text into a set of features we can analyze.  \n",
    "\n",
    "- **Tokenize** the document.\n",
    "- **Normalize** the text. \n",
    "- Compute the **term-document matrix** or **document-term matrix**.\n",
    "\n",
    "***\n",
    "**Note:** You can find additional information on the R `tm` package in the [vignette by Feinerer](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text\n",
    "\n",
    "As a first step in preparing text for analysis of a document is to **tokenize** the text. In general terms, tokenization is the process dividing raw text into words, symbols and other elements, known as **tokens**. A set of tokens from one or more documents is known as a **corpus**.\n",
    "\n",
    "As a first step in creating a corpus is reading the data set. This particular data set is comprised of 160,000 tweets. The sentement of these tweets has been human labled as positive or negative {0,4}. The code in the cell below reads the tweet text and sentiment. The sentiment is marked as {0,1} for positive and negative. Run this code to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read the tweet data set\n",
    "tweets = read.csv('Binary Classification_ Twitter sentiment analysis.csv', \n",
    "                   header = TRUE, stringsAsFactors = FALSE)\n",
    "colnames(tweets) <- c(\"sentiment\", \"tweets\") # Set the column names\n",
    "tweets[, 'sentiment'] = ifelse(tweets$sentiment == 4, 1, 0)  # set sentiment to {0,1}\n",
    "head(tweets) # Have a look at the data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data set read, we need to tokenize the tweets. The code in the cell below does the following:\n",
    "\n",
    "- The `VectorSource` function adds attributes to each document in the corpus. In this case each tweet is considered a document in a vector of documents.  \n",
    "- The each tweet text is tokenized and organized into a document within the corpus by the `Corpus` function.  \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a tm text corpus from the tweets\n",
    "library(tm)  ## tm package for text mining\n",
    "temp = VectorSource(tweets['tweets'])\n",
    "str(temp)\n",
    "tweet.corpus <- Corpus(temp)\n",
    "# What is the class of the corpus\n",
    "cat('')\n",
    "class(tweet.corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "With the corpus constructed we can perform text normalization on these documents. There are functions in tm which perform all of these steps, but for the purpose of illustration we will go step-by-step. \n",
    "\n",
    "Text normalization involves removing extrainous symbols and words, ensuring that text is uniformly coded, and converting words to their roots. The following list outlines some commonly used text normalization steps inlcuding some examples. \n",
    "\n",
    "- **Strip extra white space:** {I <3 statistics $\\ \\ $, it’s my \\u1072  $\\ \\ $    fAvoRitE!! 11!!!}\n",
    " $\\longrightarrow$ {I <3 statistics, it’s my \\u1072 fAvoRitE!! 11!!!}\n",
    "- Remove **Unicode text**: {I <3 statistics, it’s my \\u1072 fAvoRitE!! 11!!! $\\longrightarrow$ I <3 statistics, it’s my fAvoRitE!! 11!!!} \n",
    "- Convert to **lower case:** {I <3 statistics, it’s my fAvoRitE!! 11!!!$\\longrightarrow$ i <3 statistics, it’s my favorite!! 11!!!}\n",
    "- **Remove punctuation:** {i <3 statistics, it’s my favorite!! 11!!! $\\longrightarrow$ i 3 statistics its my favorite 11}\n",
    "- **Remove numbers:** {i 3 statistics its my favorite 11 $\\longrightarrow$ i statistics its my favorite}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below does the following:\n",
    "\n",
    "- Uses `tm_map` to iterate over all of the documents in the corpus.\n",
    "- The `content_transformer` function is used to transform each document. The argument to `content_transformer` specifies the type of transformation to be performed. \n",
    "\n",
    "Exectue the code to perform some basic text normalization steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Normalize tweets text\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(removeNumbers))\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(removePunctuation))\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(stripWhitespace))\n",
    "tweet.corpus <- tm_map(tweet.corpus, content_transformer(tolower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Document Matrix\n",
    "\n",
    "Now that we have a corpus with some basic normalization applied, we can create a **term document matrix (tdm)** The tdm is a representation of **Bag of Words** model. The tdm has the following properties:\n",
    "\n",
    "- Frequencies of a given term  are in the rows. The **term frequencies (TF)** for each document are in the columns\n",
    "- The tdm is a sparse matrix, as most documents do not include many of the terms. Sparse matrix coding must be used for efficiency. \n",
    "- **Document term matrix (dtm)** is transpose\n",
    "- Using the distribution of a document’s TF or **term frequency inverse document  frequency (TF-IDF)** values a number of analyses can be performed, including:\n",
    "  - Characterize writing styles\n",
    "  - Comparing authors\n",
    "  - Determining original authors\n",
    "  - Finding plagiarism\n",
    "\n",
    "Let's look at an example of a tdm. The figure below shows a corpus of text documents on the left. This corpus is transformed into the term document matrix shown on the right. Notice that the matrix is sparse as any given document may not contain a term. Additionally, some terms may appear in the document multiple times. \n",
    "\n",
    "![](img/tdm.png)\n",
    "\n",
    "The code in the cell below computes a tdm using the R `slam` sparse matrix package. Terms with very low frequency across all documents are removed from the matrix. These sparse terms are generally not informative. \n",
    "\n",
    "Run this code and examine the summary of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ----- Convert the corpus to a term document matrix\n",
    "to.tdm = function(corpus, sparse = 0.998){\n",
    "  require(tm)\n",
    "  ## Compute a term-document matrix and then \n",
    "  require(slam) # Sparse matrix package\n",
    "  tdm <- TermDocumentMatrix(corpus, control = list(stopwords = FALSE))\n",
    "  tdm <- removeSparseTerms(tdm, sparse)\n",
    "  tdm\n",
    "}\n",
    "tdm = to.tdm(tweet.corpus) # Create a term document matrix\n",
    "str(tdm) # Look at sparse tdm\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComputingTerm Frequency\n",
    "\n",
    "Now that we have computed a tdm, how can we understand it? Recall that the simple **Bag of Words model** is just based on **Term Frequency (TF)**. In this case, the weighting of a document for a given term is just the frequency of that term in the document. \n",
    "\n",
    "In other cases we will used the **Inverse Document Frequency (IDF)** weighting. IDF weighting accounts for cases where only a few documents contain certain terms. The formula for the IDF weighting can be written as:\n",
    "\n",
    "$$IDF = log(\\frac{Number\\ Documents}{Number\\ Documents\\ with\\ Word})$$\n",
    "\n",
    "The IDF can exhibit a problem however. When there are a few documents with very frequent terms, the weighting is skewed toward those documents.  To solve this problem, we reweight IDF by the overall frequency of the word to create a **term frequency-inverse document frequency (TF-IDF)** matrix. The formula for computing TFIDF is: \n",
    "\n",
    "$$TF - IDF = frequency(word) \\cdot log(\\frac{Number\\ Documents}{Number\\ Documents\\ with\\ Word})\\ $$\n",
    "\n",
    "The code in the cell below computes both simple TF and the cumulative of the term frequencies, strating from the most fequent terms to the least. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute the word fequency from the tdm\n",
    "to.wf = function(tdm){\n",
    "  ## compute the word frequencies.\n",
    "  require(slam)\n",
    "  freq <- row_sums(tdm, na.rm = T)   \n",
    "  ## Sort the word frequency and build a dataframe\n",
    "  ## including the cumulative frequecy of the words.\n",
    "  freq <- sort(freq, decreasing = TRUE)\n",
    "  word.freq <- data.frame(word = factor(names(freq), levels = names(freq)), \n",
    "                          frequency = freq)\n",
    "  word.freq['Cumulative'] <- cumsum(word.freq['frequency'])/sum(word.freq$frequency)\n",
    "  word.freq\n",
    "}\n",
    "wf = to.wf(tdm)\n",
    "head(wf, n = 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that certain common words are quite frequent. \n",
    "\n",
    "To further investigate term frequency, execute the code in the cell below to create an ordered bar plot of term frequncy and cumulative term frequency and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make a bar chart of the word frequency\n",
    "word.bar = function(wf, num = 50){\n",
    "  require(ggplot2)\n",
    "  ggplot(wf[1:num,], aes(word, frequency)) +\n",
    "    geom_bar(stat = 'identity') +\n",
    "    ggtitle('Frequency of common words') +\n",
    "    ylab('Frequency') +\n",
    "    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n",
    "}\n",
    "word.bar(wf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make cumulative distribution plots of the most frequent words\n",
    "word.cdf = function(wf, num = 50){\n",
    "  require(ggplot2)\n",
    "  ggplot(wf[1:num,], aes(word, Cumulative)) +\n",
    "    geom_bar(stat = 'identity') +\n",
    "    ggtitle('Cumulative fraction of common words') +\n",
    "    ylab('Cumulative frequency') +\n",
    "    theme(axis.text.x = element_text(angle = 90, hjust = 1))\n",
    "}\n",
    "word.cdf(wf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the frequency of terms drops off rather quickly. This is a common situation with many documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words\n",
    "\n",
    "From the foregoing analysis of word frequency in the tweets we can see that the most frequent words do not have any particular semantic meaning. We say these words are **stop words**. To compute a meaningful TF it is important to remove the stop words. An example of removing stop words is shown below:\n",
    "\n",
    "i statistics its my favorite $\\longrightarrow$ statistics favorite\n",
    "\n",
    "The choice of stop words is often dependent on the applicaton at hand. Some words may generally be stop words, but may be critical in some applicaitons. For the tweets, the range of stop words must be extended to include the non-standard spelling often used in tweets. In practice, you may need to try several choices of stop word lists to find the best list for a given problem.\n",
    "\n",
    "The code in the cell below, reads a custom list of stop words, ensures they are unique and then prints the first 100. Execute the code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load stop words from a file and ensure they are \n",
    "stopWords = read.csv('stopwords.csv', header = TRUE, stringsAsFactors = FALSE)\n",
    "stopWords = unique(stopWords) # Ensure the list is unique\n",
    "cat(nrow(stopWords))\n",
    "stopWords[1:100,] # Look at the first 100 stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These stop words contain many words which are unlikely to help us determine the sentiment of the tweets. \n",
    "\n",
    "Execute the code in the cell below to rempove the stop words from the corpus of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Remove the stop words from the corpus\n",
    "tweet.corpus <- tm_map(tweet.corpus, removeWords, stopWords[, 'words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to examine the frequency of words in the tweets with the stop words removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## View the results\n",
    "tdm = to.tdm(tweet.corpus) # Create a term document matrix\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n",
    "wf = to.wf(tdm)  # Compute word fequency\n",
    "head(wf, n = 10)  # Look at the most common words\n",
    "word.bar(wf) # Plot word frequency\n",
    "word.cdf(wf) # Plot cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the most frequent words appear to have symantic meaning and will help us determine tweet sentiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steming words\n",
    "\n",
    "Word can appear in different forms. For example, the differnt tenses of a verb are the same word, but use different spelling. Dispite the different spelling, the word has the same meaning and symantics. \n",
    "\n",
    "To ensure that the same word is treated the same in an analysis, dispite spelling differences, inflected (or sometimes derived) words are **stemmed** to their roots. \n",
    "  - Stemming was pioneered by Julie Beth Lovens (1968). \n",
    "  - The Porter Stemmer (1980, 2000) is common algorithm for stemming English words.\n",
    "  - Example 1: {relies, relied, rely = reli}\n",
    "  - Example 2: {statistics favorite $\\longrightarrow$ statisti favori}\n",
    "\n",
    "A related process is to **substitute synonyms** is sometimes necessary, but can be tricky. We will not attempt this process in our example. \n",
    "\n",
    "The code in the cell below uses the Porter Stemmer from the SnowballC package to stem the words in our corpus. Exectue this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use the porter stemmer in Snowball package\n",
    "require(SnowballC) ## For Porter stemming words\n",
    "tweet.corpus <- tm_map(tweet.corpus, stemDocument)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code in the cell below to examine the differences in the word frequency of the tweets following stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## View the results\n",
    "tdm = to.tdm(tweet.corpus, sparse = 0.99) # Create a term document matrix\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n",
    "wf = to.wf(tdm)  # Compute word fequency\n",
    "head(wf, n = 10)  # Look at the most common words\n",
    "word.bar(wf) # Plot word frequency\n",
    "word.cdf(wf) # Plot cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## View the results\n",
    "tdm = to.tdm(tweet.corpus, sparse = 0.99) # Create a term document matrix\n",
    "findFreqTerms(tdm, 2000) # Words that occur at least 2000 times\n",
    "wf = to.wf(tdm)  # Compute word fequency\n",
    "head(wf, n = 10)  # Look at the most common words\n",
    "word.bar(wf) # Plot word frequency\n",
    "word.cdf(wf) # Plot cdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Clouds\n",
    "\n",
    "Word clouds are a completely useless display of information that people love to see.\n",
    "\n",
    "![](img/Wordcloud.png)\n",
    "\n",
    "Beware of any presentation using word clounds!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Sentiment Analysis\n",
    "\n",
    "Now that we have a prepared TDM of the 160,000 tweets, let's build and evaluate models to classify the sentiment of these tweets. An outline of our process is as follows:\n",
    "\n",
    "- Use TDM or TF-IDF weighted TDM as features for training the model.\n",
    "- Use marked cases for training and evaluation of model.\n",
    "- Slect a method for sparse matrix requires regularization from the following:\n",
    "  - Feature selection, is impractial since there are over one million features.\n",
    "  - SVD/PCA could be used to reduce dimensionality of the problem.\n",
    "  - In this case we will use the ridge and lasso methods offered in the  elasticnet model.\n",
    "\n",
    "***\n",
    "**Note:** For an in depth discussion of supervised classification of documents using the R `RTextTools` package, see the [2013 article by Jurka et. al.](https://journal.r-project.org/archive/2013-1/collingwood-jurka-boydstun-etal.pdf).\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below uses the `create_matrix` function from the RTextTools package to create a TDM. The `create_container` function is used to package the model matrix and labels for 120,000 training cases. The remaining  40,000 cases will be used to evaluate the model.\n",
    "\n",
    "Execute the code below. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute a tdm\n",
    "require(RTextTools)\n",
    "model.matrix = create_matrix(tweets$tweets, language=\"english\",                               \n",
    "                           removeNumbers=TRUE,\n",
    "                           stemWords=TRUE, \n",
    "                           removeSparseTerms=.998, \n",
    "                           removeStopwords = TRUE, \n",
    "                           stripWhitespace = TRUE,\n",
    "                           toLower = TRUE)                            \n",
    "\n",
    "## Create the a container for the tdm and label\n",
    "tweet.cont = create_container(model.matrix, \n",
    "                              tweets$sentiment, \n",
    "                              trainSize = 1:120000, \n",
    "                              virgin=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the  code in the cell below to train a glmnet model on the 120,000 training cases in the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute a logistic regresson model for sentiment classification\n",
    "tweet.glmnet <- train_model(tweet.cont, \"GLMNET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below scores the 40,000 tweets not used to train the  model and computes model performance metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test classification\n",
    "tweet.class = classify_model(tweet.cont, tweet.glmnet)\n",
    "tweet.metrics = create_analytics(tweet.cont, tweet.class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code in the cell below to display the classification of the tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Examine some raw metrics\n",
    "tweet.metrics@label_summary\n",
    "cbind(head(tweet.metrics@document_summary, n = 10), head(tweets$sentiment, n = 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coded in the cell below commputes the precisio, recall and Fscore of the model for positive and negative tweets. Execute the code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_precisionRecallSummary(tweet.cont, tweet.class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These figures are not particularly good. Perhaps we can do model using a TFIDF TDM. Exectue the code in the cell below to do just this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "## Compute TFIDF weighted tdm\n",
    "## Compute a tdm\n",
    "tdm.tools2 = create_matrix(tweets$tweets, \n",
    "                           language=\"english\",                               \n",
    "                           removeNumbers=TRUE,\n",
    "                           stemWords=TRUE, \n",
    "                           removeSparseTerms=.998, \n",
    "                           removeStopwords = TRUE, \n",
    "                           stripWhitespace = TRUE,\n",
    "                           toLower = TRUE,\n",
    "                           weighting = tm::weightTfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to create a container with 120,000 training cases from the TFIDF TDM. As before, this may take some  time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create the a container for the TfIdf weighted tdm and label\n",
    "tweet.cont = create_container(tdm.tools2,tweets$sentiment, trainSize = 1:120000, virgin=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to compute the new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute a logistic regresson model for sentiment classification\n",
    "tweet.glmnet.TfIdf <- train_model(tweet.cont,\"GLMNET\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code in the cell below to classify the test cases and compute the model performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Test classification\n",
    "tweet.class.TfIdf = classify_model(tweet.cont, tweet.glmnet.TfIdf)\n",
    "tweet.metrics.TfIdf = create_analytics(tweet.cont, tweet.class.TfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute code in the cell below to see some of the raw labels and scores for the test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Examine some raw metrics\n",
    "tweet.metrics.TfIdf@label_summary\n",
    "results = head(tweet.metrics.TfIdf@document_summary, n = 20)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the code in the cell below to display the performance metrics for both models. Examine and compare the reuslts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_precisionRecallSummary(tweet.cont, tweet.class.TfIdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Models\n",
    "\n",
    "It is often useful to allocate documents to one or more topics. This process can be useful in, say, information retrival and search. Models to perform this allocation to topics are known as **topic models**. \n",
    "\n",
    "A powerful topic model is know as **Latent Dirichlet Allocation** or **LDA**. LDA is an unsupervised Bayesian learning model.  We can summarize the LDA model as follows:\n",
    "\n",
    "- The LDA model uses a fixed number of (sub) topics, k.\n",
    "- The model computes the posterior probability of a document containing a topic.\n",
    "- The model uses know word frequencies for documents in corpus, e.g. the tdm.\n",
    "- All other variables are estimated or **latent**, including the topics of each document. \n",
    "\n",
    "How does Latent Dirichlet alloction work? It's a Baysian model, so we need to define a likeihood and choose a prior. \n",
    "\n",
    "Our posterior distribution is categorical, since we have many topics. The Dirichlet distribution is the conjugate of the multinomial and categorical distributions.\n",
    "\n",
    "All we actually know: $w_{ij}$ is the frequency of a specific word $j$ in document $i$.\n",
    "\n",
    "What we want to know (latent): $\\theta_i$ is the topic distribution of document $i$.\n",
    "\n",
    "We also need to estimate (latent):\n",
    "- $\\phi_k$ is the word distribution for topic $k$\n",
    "- $z_{ij}$ is the topic of the jth word in document $i$\n",
    "\n",
    "The Bayesian model and its priors:\n",
    "\n",
    "- Multinomial model\n",
    "$$z_{ij} \\sim Multinomial(\\theta_i)\\\\\n",
    "w_{ij} \\sim Multinomial(\\phi_k)$$\n",
    "\n",
    "- With Dirichlet priors with parameters $\\alpha$ and $\\beta$:\n",
    "$$\\theta_i \\sim Dirichelet(\\alpha)\\\\\n",
    "\\phi_k \\sim Dirichelet(\\beta)$$\n",
    "\n",
    "Since we don't know the allocation of topics in advance we generally use uniform priors across topics.\n",
    "\n",
    "The Likelihood is taken from the TD matrix.\n",
    "\n",
    "***\n",
    "**Note:** An in depth introduction to fitting topic models with the R `topicmodels` package can be found in the [vignette by Grun and Hornik](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an example. In this example we apply LDA to a corpus of 20 business news articles from Reuters news wire concering the oil industry. We will apply an LDA model with 5 topics ($k = 5$). \n",
    "\n",
    "As a first step we will load the corpus of these docments. Execute the code in the cell below to load the corpus and examine the contents of the first document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load the data set as a vector corpus of 20 documents\n",
    "library(tm)\n",
    "data(crude)\n",
    "writeLines(as.character(crude[[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the corpus is loaded, let's create a term document matrix. The code in the cell below does just this, including text normalization. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Compute the term document matrix\n",
    "crude.tdm = TermDocumentMatrix(crude, control = list(removePunctuation = TRUE,\n",
    "                                                     tolower = TRUE,\n",
    "                                                     removePunctuation = TRUE,\n",
    "                                                     removeNumbers = TRUE,\n",
    "                                                     stopwords = TRUE,\n",
    "                                                     stemming = TRUE))\n",
    "## Have a look at the tdm \n",
    "inspect(crude.tdm[202:210, 1:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the term frequencies for the selcted words and documents in the display above. As expected, most of the entries in the tdm are zeros. A few words occur more than once. \n",
    "\n",
    "To further examine the occurance of the words in these documents, let's find words which occur 10 or more times. The `findFreqTerms` function does just that. Execute the code in the cell below to see the list of most common terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Which terms occur 10 times or more?\n",
    "crudeTDMHighFreq <- findFreqTerms(crude.tdm, 10, Inf)\n",
    "crudeTDMHighFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To furter investigate the distribution of these most frequent terms, we can create a tdm for them for the first 5 documents in the corpus. Execute the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do these terms show up in the first 10 documents?\n",
    "inspect(crude.tdm[crudeTDMHighFreq, 1:20]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for these most frequent terms the tdm is sparse. \n",
    "\n",
    "The LDA model actually uses a DTM (as opposed to the TDM). Execute the code in the cell below to compute the DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute the DTM\n",
    "crude.dtm = DocumentTermMatrix(crude, control = list(removePunctuation = TRUE,\n",
    "                                                     stopwords = TRUE))\n",
    "crude.dtm  ## Check the drm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are about 1,700 non zero entries. 91% of the entries in the DTM are zero. \n",
    "\n",
    "Now, we are ready to apply the LDA model to the DTM. The code in the cell below does the following:\n",
    "\n",
    "- Define parameters for the Gibbs sampler used to compute the posterior distribution.\n",
    "- Set the number of topics for the model.\n",
    "- Compute the posterior distribution of the topics. \n",
    "\n",
    "Exectue this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Apply a topic model to the news articles\n",
    "##load topic models library\n",
    "library(topicmodels)\n",
    "\n",
    "#Set parameters for Gibbs sampling\n",
    "burnin <- 4000\n",
    "iter <- 2000\n",
    "thin <- 500\n",
    "seed <-list(2003,5,63,100001,765)\n",
    "nstart <- 5\n",
    "best <- TRUE\n",
    "\n",
    "#Number of topics\n",
    "k <- 5\n",
    "\n",
    "ldaOut = LDA(crude.dtm, k, method= \"Gibbs\", \n",
    "             control = list(nstart = nstart, \n",
    "                            seed = seed, \n",
    "                            best = best, \n",
    "                            burnin = burnin, \n",
    "                            iter = iter, \n",
    "                            thin=thin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's examine thhe properties of  this model. Execute the code in the cell below to see the most likely topic for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Examine the topics\n",
    "ldaOut.topics <- as.matrix(topics(ldaOut))\n",
    "ldaOut.topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can examine the most frequent terms for each of our 5 topics. Exectue the code in the cell below to create a chart of the most frequent terms in each of the 5 topics found by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## And the terms\n",
    "ldaOut.terms <- as.matrix(terms(ldaOut,6))\n",
    "head(ldaOut.terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#probabilities associated with each topic assignment\n",
    "topicProbabilities <- as.data.frame(ldaOut@gamma)\n",
    "head(topicProbabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find relative importance of top topic\n",
    "topic1ToTopic2 <- lapply(1:nrow(crude.dtm),function(x)\n",
    "  sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])\n",
    "unlist(topic1ToTopic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Find relative importance of second most important topics\n",
    "topic2ToTopic3 <- lapply(1:nrow(crude.dtm),function(x)\n",
    "  sort(topicProbabilities[x,])[k-1]/sort(topicProbabilities[x,])[k-2])\n",
    "unlist(topic2ToTopic3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Measuring Text Distance\n",
    "\n",
    "Measuring the distance between words in a document is not as stright forward as it might seem. The choice of distance metric can have a significant effect on analytical results. This is particularly the case for unsupervised learning methods like cluster models. \n",
    "\n",
    "Let's look at a few of the commonly used distance metrics. \n",
    "\n",
    "**Hamming Distance**\n",
    "- Line up strings, count number of positions that are the different.\n",
    "- Assumes strings are of the same length.\n",
    "\n",
    "$$𝐻𝑎𝑚𝑚𝑖𝑛𝑔(101101, 100011)=3\\\\\n",
    "𝐻𝑎𝑚𝑚𝑖𝑛𝑔(𝑏𝑒𝑒𝑟,𝑏𝑒𝑎𝑟)=1$$\n",
    "\n",
    "**Levenshtein distance** measures the edit distance between two strings (insertion, deletion, substitution only):\n",
    "\n",
    "$$𝐿𝑒𝑣(𝑏𝑒𝑒𝑟,𝑏𝑒𝑎𝑟)=1\\\\\n",
    "𝐿𝑒𝑣(𝑏𝑎𝑛𝑎𝑛𝑎,𝑏𝑎𝑛)=3$$\n",
    "\n",
    "**Jaccard index** measures the size of intersection of characters divided by size of union of characters.\n",
    "\n",
    "$$J(A, B) = 1 - \\frac{|A \\cap B|}{|A \\cup B|}\\\\\n",
    "J(beer, beer) = 1 - \\frac{3}{4}\\\\\n",
    "J(bannana, ban) = 1 - \\frac{3}{3} \\leftarrow\\ This\\ is\\ a\\ problem$$\n",
    "\n",
    "**Weighted Jaccard Index** For each letter, calculate the minimum times it appears, $m_i$, and the maximum number of times it appears, $M_i$.\n",
    "\n",
    "$$J'(A, B) = 1 - \\frac{\\sum m_i}{\\sum M_i}\\\\\n",
    "J'(beer, bear) = 1 - \\frac{m_a + m_e + m_b + m_r}{M_a + M_e + M_b + M_r}\\\\\n",
    "J'(beer, bear) = 1 - \\frac{0 + 1 + 1 + 1}{1 + 1 + 2 + 1}\\\\\n",
    "J'(bannana, ban) = 1 - \\frac{1 + 1 + 1}{3 + 1 + 2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
