{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Modeling and Markov Chain Monte Carlo\n",
    "\n",
    "### Data Science 350\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces you to a general and flexible form of Bayesian modeling using the **Makov chain Monte Carlo** methods. \n",
    "\n",
    "![](img/Flips.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Review of Bayes Theorem\n",
    "\n",
    "Bayes theorem allows us \n",
    "\n",
    "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "This is a bit of a mess. But fortunately, we don't always need the denominator. We can rewrite Bayes Theorem as:\n",
    "\n",
    "$$𝑃(𝐴│𝐵)=𝑘∙𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "Ignoring the normalizaton constant $k$, we get:\n",
    "\n",
    "$$𝑃(𝐴│𝐵) \\propto 𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "### Bayesian parameter estimation\n",
    "\n",
    "How to we interpret the relationships shown above? We do this as follows:\n",
    "\n",
    "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
    "Or\\\\\n",
    "𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠│𝑑𝑎𝑡𝑎) \\propto 𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) $$\n",
    "\n",
    "These relationships apply to the observed data distributions, or to parameters in a model (partial slopes, intercept, error distributions, lasso constant,…). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist by Bayesian models\n",
    "\n",
    "Let's summarize the differences between the Baysian and Frequentist views. \n",
    "\n",
    "- Bayesian methods use priors to quantify what we know about parameters.\n",
    "- Frequentists do not quantify anything about the parameters, using p-values and confidence intervals to express the unknowns about parameters.\n",
    "\n",
    "Recalling that both views are useful, we can contrast these methods with a chart.\n",
    "\n",
    "![](img/FrequentistBayes.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comp.like.2 = function(x, mu, sigma){\n",
    "    l = matrix(0, nrow = length(mu), ncol = length(sigma))\n",
    "    sigmaSqr = sd(x)^2\n",
    "    xBar = mean(x)\n",
    "    cat(' Mean =', xBar, 'Standard deviation =', sqrt(sigmaSqr), '\\n')\n",
    "    for(i in 1:length(sigma)){\n",
    "        sigmaSqr = sigma[i]^2\n",
    "        l[, i] = sapply(mu, \n",
    "                        function(u) exp(- n* (xBar - u)^2 / (2 * sigmaSqr)))\n",
    "    }\n",
    "    l / sum(l) # Normalize and return\n",
    "}\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Sampling and Scalability\n",
    "\n",
    "Real-world Bayes models have large numbers of parameters, even into the millions. As a naive approach to Bayesian analysis would be to simply grid sample across the dimensions of the parameter space. However, grid sampling will not scale. To underestand the scaling problem, do the following thought experiment, where each dimension is sampled 100 times:\n",
    "\n",
    "- For a 1-parameter model: $100$ samples.\n",
    "- For a 2-parameter model: $100^2 = 10000$ samples.\n",
    "- For a 3-parameter model: $100^3 = 10^6$ samples.\n",
    "- For a 100-parameter model: $100^100 = 100^{200}$ samples. \n",
    "\n",
    "As you can see, the compuational complexity of grid sampling has **exponential scaling** with dimensionality. Clearly, we need a better approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Markov Chain Monte Carlo\n",
    "\n",
    "Large-scale Bayesian models use a family of efficient sampling methods known as **Markov chain Monte Carlo sampling**. MCMC methods are compuationally efficient, but requires some effort to understand how it works and  what to do when things go wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is a Markov process?\n",
    "\n",
    "As you might guess, a MCMC sampling uses a chain of **Markov sampling processes**. The chain is built from a sequence of individul Markov processes. A Markov process is any process that a makes a transition from one state other states with probability $\\Pi$ with **no dependency on past states**. In summary, a Markov process has the  following properties:\n",
    "- $\\Pi$  only depends on the current state\n",
    "- Transition to one or more other states\n",
    "- Can ‘transition’ to current state\n",
    "- A matrix $\\Pi$ of dim N X N for N possible states\n",
    "- A Markov procecss is a random walk since any possible transition can occur from each state.\n",
    "\n",
    "A Markov chain is a sequence of Markov transition processes:\n",
    "\n",
    "$$P(X_{t + 1}| X_t = x_t, \\ldots, x_0 = x_t) = p(X_{t + 1}| x_t)$$\n",
    "\n",
    "We say the the Markov process is **memoryless**. The transition probability only depends on the current state, not any previous state. \n",
    "\n",
    "For a system with $N$ possible states we can write the transition matrix $\\Pi$ for the probaility of transition from one state to another:\n",
    "\n",
    "$$\\Pi = \n",
    "\\begin{bmatrix}\n",
    "\\pi_{1,1} & \\pi_{1,2} & \\cdots & \\pi_{1, N}\\\\\n",
    "\\pi_{2,1} & \\pi_{2,2} & \\cdots & \\pi_{2,N}\\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "\\pi_{N,i} & \\pi_{N,2} & \\cdots & \\pi_{N,N}\n",
    "\\end{bmatrix}\\\\\n",
    "where\\\\\n",
    "\\pi_{i,j} = probability\\ of\\ transition\\ from\\ state\\ i\\ to state\\ j\\\\\n",
    "and\\\\\n",
    "\\pi_{i,i} = probability\\ of\\ staying\\ in\\ state\\ i\\\\\n",
    "further\\\\\n",
    "\\pi_{i,j} \\ne \\pi_{j,i}\\ in\\ general\n",
    "$$\n",
    "\n",
    "Notice that none of these probabilities depend on the previous state history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC and the Metropolis Algorithm\n",
    "\n",
    "The first MCMC sampling algorithm is the **Metropolis Hastings algorithm** (Metropolis et al. (1953), Hastings (1970)). This algorithm is often referred to as the Metropolis algorithm. The Metropolis algorithm has the following steps to estimate the density of the likelihood of the parameters:\n",
    "1. Pick a starting point in your parameter space and evaluate the posterior according to your model. In other words, take an initial sample of the likelihood $p(data|parameters)$.\n",
    "2. Choose a nearby point in parameter space randomly and evaluate the likelihood at this point.\n",
    "  - If the $p(data | parameters)$ of the new point is greater than your current point, accept new point and move there.\n",
    "  - If the $p(data | parameters)$ of the new point is less than your current point, only accept with probability according to the ratio:  \n",
    "$$Acceptance\\ probability\\ = \\frac{p(data | new\\ parameters)}{p(data | previous\\ parameters)}$$.\n",
    "3. Repeat step 2 many times.\n",
    "\n",
    "\n",
    "Now that we have outlined the basic Metropolois MCMC algorithm, let's \n",
    "\n",
    "M-H algorithm eventually converges to the underlying distribution.\n",
    "We only have to visit N points, not 1 Trillion points.\n",
    "There is high serial correlation in M-H chain, which slows convergence\n",
    "Need to ‘tune’ the state selection probability distribution used to find the next point\n",
    "E.g. if we use Normal distribution need to pick s.\n",
    "If s is too small chain will only search the space slowly. \n",
    "If s is too big, get large jumps and slow convergence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(x)} x^{-\\alpha - 1} \\exp\\bigg(-\\frac{\\beta}{x}\\bigg)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dinvgamma = function(alpha, beta, x){\n",
    "    \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
