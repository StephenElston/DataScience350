{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Time Series Anaysis\n",
    "\n",
    "### Data Science 350\n",
    "### Stephen Elston\n",
    "\n",
    "## Introduciton\n",
    "\n",
    "This notebook provides an overview of time series analysis. Time series are an extremely common data type. Just a few of the many applications of time series analysis include:\n",
    "\n",
    "- **Demand forecasting:** Electricity production, Internet bandwidth, Traffic management, Inventory management\n",
    "- **Medicine:** Time dependent treatment effects, EKG, EEG\n",
    "- **Engineering and Science:** Signal analysis, Analysis of physical processes\n",
    "- **Capital markets and economics:** Seasonal unemployment, Price/return series, Risk analysis\n",
    "\n",
    "This notebook covers the  following topics:\n",
    "\n",
    "- Basic properties of time series.\n",
    "- Decomposition of time series.\n",
    "- Modeling of time series residuals and the ARIMA model.\n",
    "- Forecasting with the R `forecast` package. \n",
    "\n",
    "As you work with time series keep in mind the wise words of the famous american baseball player and team manager, Yogi Berra; \n",
    "\n",
    "<center> **“It's tough to make predictions, especially about the future.”**!</center>\n",
    "\n",
    "***\n",
    "**Note** To run this notebook, you must install the following packages:\n",
    "- forecast\n",
    "- repr\n",
    "***\n",
    "\n",
    "\n",
    "## Short History of Time Series Analysis\n",
    "\n",
    "The history of time series analysis starts with the pionering work of George Udny Yule (published 1927) and Gilber Walker (Published 1931). Both Yule and Walker worked on the auto regresive (AR) model for stochastic time series.\n",
    "\n",
    "![](img/George_Udny_Yule.jpg)\n",
    "<center> **George Yule; time series pioner**\n",
    "\n",
    "Mathematical prodegee, Norbert Weiner invented filters for stochastic time series processes during the Second World war. Weiner worked at MIT and was assigned to a project to immrove the accuracy of anti-aircraft guns using the noisy radar signals of the day. He published his seminal paper on the subject in 1949. If you have recently used a mobile phone or streamed video or audio you are reciving benefits of wiener's research!\n",
    "\n",
    "![](img/Norbert_wiener.jpg)\n",
    "<center> **Norbert Weiner: Invented time series filter**\n",
    "\n",
    "George Box and Gwilym Jenkins fully developed the the statistical theory  of time series by extending the work of Yule and Walker in the 1950s and d 1960s. This work was fully developed in thier seminal 1970  book. Their theory included the auto regressive moving averge (ARMA) model and the auto regressive integrfated moving averge (ARIMA) models we use in this notebook.\n",
    "\n",
    "George Box was maired to Joan Fisher Box, an outstanding statistician in her own right and daughter of Ronald Fisher. \n",
    "\n",
    "![](img/GeorgeEPBox.jpg)\n",
    "<center> **George Box fully developed the ARIMA model**\n",
    "\n",
    "![](img/BoxJenkins.jpg)\n",
    "<center> **Seminal book: by Box and  Jenkins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "The book by Paul Cowpertwait and Andrew Metcalfe, *Introductory Time Series with R*,Springer, 2009,is good introduction to time series analysis. This book includes copious examples using R. \n",
    "\n",
    "![](img/Cowpertwait.jpg)\n",
    "\n",
    "A more comprehensive treatment of time series theory can be found in a number of books. *Time Series Analysis With Applications in R* by Jonathan Cryer and Kung-Sik Chan, Springer, 2010, is both comprehensive and readable. However, the R code examples are sparse and poorly organized.\n",
    "\n",
    "![](img/Cryer.jpg)\n",
    "\n",
    "A comprehensive list of R time series packages can be found in the [CRAN Time Series Task View](https://cran.r-project.org/web/views/TimeSeries.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Time Series in R\n",
    "\n",
    "R contains a number of time series classes. In this notebook we will work with the basic `ts` class. A powerful `zoo` class provides more capabilities. The `xts` package adds extensibility to the `zoo` class. Many other R time series classes are available. \n",
    "\n",
    "Let's create a simple R `ts` class time series object. Execute the code in the cell below to create a vector of values from a `sin` function. Note the class of the resulting vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vec = sin((1:365)/30)  # A vector of values\n",
    "class(vec) # Vector is an atomic class in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have creted a numeric vector.\n",
    "\n",
    "The code in the cell below does the following:\n",
    "\n",
    "- The class of the vector is converted to `ts` with attributes of a start date and a frequecy.\n",
    "- The class and attributes of the of the time series are printed.\n",
    "- The time series is plotted using the `ts` plot method.\n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create a ts class object from the vector\n",
    "## by adding time series attributes\n",
    "vec.ts = ts(vec, start = 1990/01/01, freq = 365)\n",
    "attributes(vec.ts) # Note the time series attributes\n",
    "require(repr)\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "plot(vec.ts) # Note the x-axis is the time attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector is now of class `ts`. The attributes include the start time, end time and frequency. The plot shows the time series values on the vertical axis and the calendar date on the horizontal axis. \n",
    "\n",
    "In summary, a time series object is an ordinary vector, matrix of data frame with special attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Time Series Properties\n",
    "\n",
    "In this section we will explore some basic properties time series. Underrstnding these properties will help in understanding the models we explore in subsequent sections.\n",
    "\n",
    "### Properties of White Noise Series\n",
    "\n",
    "A random series drawn from **independent identically distributed (iid)** noise drawen from a Normal distribution. Such a series is said to be a **white noise** series. Since the series is iid there is no correlation from one value to the next. We can write a **discrete** white noise time series as just:\n",
    "\n",
    "$$X(t) = (w_1, w_2, w_3, \\dots, w_n)\\\\\n",
    "where\\\\\n",
    "w_t = N(0, \\theta)$$\n",
    "\n",
    "Notice that the standard deviation and therefore the variance of the series, $\\theta$, is constant in time. We say that a time series with a constant variance is **stationary**. The properties of a stationary time series do not vary with time. \n",
    "\n",
    "Further, the values of the time series are given at specific or discrete times, making this a discrete time series. In compuational time series analysis we nearly always work with discrete time series. Some time series are inherently discrete including, unemployment rate average over a month, the daily closing price of a stock. Even if the underlying time series is continious, we typically work with **values sampled at discrete points in time**. For example, temperature is a continious variable, but we will generally work with sampled variables, such as hourly measurements. \n",
    "\n",
    "The code in the cell below creates a time series from an iid Normal distribution with mean zero. Execute this code and note the attributes and the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "ts.white = function(n, mean = 0.0, sd = 1.0, start = 1990, freq = 12){\n",
    "  ts(rnorm(n, mean = mean, sd = sd), start = start, freq = 12)\n",
    "}\n",
    "white = ts.white(180)\n",
    "attributes(white)\n",
    "plot(white)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values of the time series move wander randomly around zero, with no particular trend. \n",
    "\n",
    "Next, let's look at the distribution of the time series values. The code in the cell below plots the histogram and Q-Q Normal plot of the values of the time series. Run this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist.ts = function(df, col = 'residual', bins = 40){\n",
    "  par(mfrow = c(1,2))\n",
    "  temp = as.vector(df)\n",
    "  breaks = seq(min(temp), max(temp), length.out = (bins + 1))\n",
    "  hist(temp, breaks = breaks, main = paste('Distribution of ', col), xlab = col)\n",
    "  qqnorm(temp, main = paste('Normal Q-Q plot of ', col))\n",
    "  par(mfrow = c(1,1))\n",
    "}\n",
    "dist.ts(white, col = 'white noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the values of the white noise series are normalilly distributed. When examining these plots keep in mind there are only 180 values. \n",
    "\n",
    "The values of the white noise series are iid, so we do not expect the values to show any dependency over time. In time series analysis we measure dependency using **autocorrelation**. Autocorrelation is the correlation of a series with itself lagged (off set) by some number of time steps. Autocorrelation at lag k can be writen as:\n",
    "\n",
    "$$\\rho_k = \\frac{\\gamma_k}{n \\sigma^2} = \\frac{1}{n \\sigma^2} {\\Sigma_{t = 1}^N (x_{t} - \\mu) \\cdot (x_{t-k} - \\mu)}\\\\\n",
    "where\\\\\n",
    "k = lag\\\\\n",
    "\\gamma_k = covariance\\ lag\\ k\\\\\n",
    "\\mu = mean\\ of\\ the\\ series\\\\\n",
    "\\sigma^2 = variance\\ of\\ the\\ series = \\frac{1}{n-1}\\Sigma_{t = 1}^N (x_{t} - \\mu) \\cdot (x_{t} - \\mu)$$\n",
    "\n",
    "Notice that for any series, $\\rho_0 = 1$. in other words, the autocorrleation of a series at lag zero equals one. \n",
    "\n",
    "We can also define a second order **partial autocorrelation**. The Partial autocoorelation at lag k is the correlation that results from removing the effect of any correlations due to the terms at smaller lags.\n",
    "\n",
    "Let's plot the autocorrelation function (acf) and partial autocorrelation function (pacf) of the white noise series. Run the code in the cell below to compute and plot these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.pmales.extlot.width=8, repr.plot.height=6)\n",
    "plot.acf <- function(df, col = 'remainder', is.df =TRUE){\n",
    "  if(is.df) temp <- df[, col]\n",
    "  else temp <- df\n",
    "  par(mfrow = c(2,1))\n",
    "  acf(temp, main = paste('ACF of', col))\n",
    "  pacf(temp, main = paste('PACF of', col))\n",
    "  par(mfrow = c(1,1))\n",
    "}\n",
    "      plot.acf(white, col = 'white noise', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the white noise series only has a significant value at lag zero. There are no significant partial autocorrelation values. The dotted line is the 95% confidence interval. \n",
    "\n",
    "**Your Turn:** In the cell below create and execute the code to create a monthly series `(freq = 12)` of white noise plus a sinusodial component. The white noise component should have mean 0 and standard deviation 0f 1.0. Create a time series plot of the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walks\n",
    "\n",
    "A **random walk** is the defined by the sum of a white noise series. In other words, the value of the random walk is the cumulative sum of the preceeding white noise series. \n",
    "\n",
    "$$y_t = y_{t-1} + w_t\\\\\n",
    "Or\\\\\n",
    "w_t = y_t - y_{t-1}$$ \n",
    "\n",
    "The quantities $y_t - y_{t-1}$ are known as the **innovations** of the random walk.\n",
    "\n",
    "But note that the covariance of a random walk increases with time and is not bounded.\n",
    "\n",
    "$$\\gamma_k = Cov(x_t, x_{t+k}) = t \\sigma^2 \\rightarrow \\infty\\ as\\ t \\rightarrow \\infty$$\n",
    "\n",
    "Therefore, the random walk is **not stationary** . \n",
    "\n",
    "The code in the cell below computes a random walk series by taking the cumulative sum over Noramlly distributed inovations. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Investigate the time series properties of random walk\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "ran.walk = function(n, freq = 12, start = 1990, sd = 1.0, mean = 0.0){\n",
    "  norms = rnorm(n, mean = mean, sd = sd)\n",
    "  ts(cumsum(norms), start = start, freq = 12)\n",
    "}\n",
    "ranWalk = ran.walk(180)\n",
    "plot(ranWalk, main = 'Random walk time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random walk wanders back and forth, but eventually heads in a particular direction. \n",
    "\n",
    "**Your Turn** What does the distribution of values of the random walk look like. What about the ACF and PACF of the random walk? In the cell below, create and execute the code to examine the probability distribution, ACF and PACF of the radom walk. Answer the following questions:\n",
    "\n",
    "- How close to Normally distributed are the values of the random walk?\n",
    "- How are the properties of the ACF and PACF different from those of the white noise series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Noise Series with Trend\n",
    "\n",
    "What happens when we add a trend to the white noise series? Run the code in the cell below adds a linear trend to a white noise series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ---- Investigate time series properties of \n",
    "## trend + white noise\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "ts.trend = function(n, slope = 0.01, mean = 0.0, sd = 1.0, start = 1990, freq = 12){\n",
    "  temp = seq(0, slope * n, length.out = n) + \n",
    "          rnorm(n, mean = mean, sd = sd)\n",
    "  ts(temp, start = start, freq = 12)\n",
    "}\n",
    "trend = ts.trend(180, slope = 0.05)\n",
    "plot(trend, main = 'Trend + white noise time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the time series trends upward with a linear trend with iid Normal deviations. \n",
    "\n",
    "Run the code in the cell below to examine the distribution of values in this time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dist.ts(trend, col = 'trend + white noise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is mostly Normal, but with heavy tails. \n",
    "\n",
    "How does adding a trend change the ACF and PACF? Run the code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(repr.pmales.extlot.width=8, repr.plot.height=6)\n",
    "plot.acf(trend, col = 'trend + white noise', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the that the ACF decays slowly, as was the case with the random walk. In additon, the PACF shows significant values for at least one lag. This is the result of the trend creating dependency from one value to the next. Any time series with a trend is **not stationary**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series with a Seasonal Component\n",
    "\n",
    "Many real-world tine series include a seasonal component. A seasonal component is a period variation in the values of the time series. The periods can be measured in years, months, days, days of the week, hours of the day, etc. Some examples of seasonal components of time series inclde:\n",
    "\n",
    "- Opion expiration dates in capital markets.\n",
    "- Anual holidays which can affect transportation, utility use, shopping habits, etc.\n",
    "- Weekend vs. business days, which account for volumns of certian transaction behavior.\n",
    "- Month of the year which can affect emploiyment patterns, weather, etc.\n",
    "\n",
    "Let's investigate the properties of a time series with a seasonal component. The coded in the cell below creates and plots a time series with sinusoidal seasonal component added to White noise. Run this coded and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## --- Investigate time series properties of \n",
    "## trend + white noise + seasonal\n",
    "ts.season = function(n, slope = 0.01, mean = 0.0, sd = 1.0, start = 1990, freq = 12){\n",
    "   temp = seq(0, slope * n, length.out = n) + \n",
    "    rnorm(n, mean = mean, sd = sd) +\n",
    "    2 * sin(0:(n -1) * pi / freq) +\n",
    "     cos(0:(n -1) * pi / freq)\n",
    "  ts(temp, start = start, freq = 12)\n",
    "}\n",
    "season = ts.season(180, slope = 0.00)\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "plot(season, main = 'White noise + seasonal time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the time series looks like a noisy sin wave.\n",
    "\n",
    "**Your Turn** What does the distribution of values of the seasonal time series look like. What about the ACF and PACF of the sesonal time series? In the cell below, create and execute the code to examine the probability distribution, ACF and PACF of this time series. Answer the following questions:\n",
    "\n",
    "- How close to Normally distributed are the values of the seasonal time series?\n",
    "- How are the properties of the ACF and PACF different from those of time series you have already examined?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition of Time Series\n",
    "\n",
    "We have looked at the properties of several types of time series. \n",
    "\n",
    "- White noise series.\n",
    "- Random walks.\n",
    "- White noise series with trend.\n",
    "- White noise series with seasonal component.\n",
    "\n",
    "Next, we have to look into methods for decomposing time series data into its **trend, seasonal and residual** components.\n",
    "\n",
    "### The STL Decomposition Models\n",
    "\n",
    "A direct decomposition model is know as the **seasonal, trend and residual** or **STL** model. Not too surprisingly this model does the following:\n",
    "\n",
    "- The trend is removed using a LOESS regression model. \n",
    "- The seasonal component is removed using a regression on periodic components.\n",
    "- The remainder is know as the residual. \n",
    "\n",
    "The decomposition can be either additive or multiplicative. The additive model simply sums the componets and is writen:\n",
    "\n",
    "$$TS(t) = S(t) + T(t) + R(t)$$\n",
    "\n",
    "The multiplicative model multiplies the three components. This model is particuarly useful in the common case where the seasonal effect increases in proportional to the trend. We can write this  model as follows:  \n",
    "\n",
    "$$TS(t) = S(t)\\ *\\ T(t)\\ *\\ R(t)\\\\\n",
    "or\\\\\n",
    "log(TS(t)) = S(t) + T(t) + R(t)$$\n",
    "\n",
    "You can find details of this model in [Rob Hyndman's lecture notes](http://robjhyndman.com/uwafiles/5-Cross-validation.pdf). \n",
    "\n",
    "Let's try this out on a time series which has a seasonal, trend and white noise residual component. Run the code below to compuute the model and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Decomposition of the time series into components\n",
    "ts.decomp <- function(df, col = 'elec.ts', span = 0.5, Mult = TRUE, is.df = TRUE){\n",
    "  # if(Mult) temp = log(df[, col])  else temp = ts(df[, col]\n",
    "  if(is.df) temp = log(df[, col])  \n",
    "  else temp = df\n",
    "  spans = span * length(temp)  \n",
    "  fit <- stl(temp, s.window = \"periodic\", t.window = spans)\n",
    "  plot(fit, main = paste('Decompositon of',col,'with lowess span = ', as.character(span)))\n",
    "  fit$time.series\n",
    "}\n",
    "season.trend = ts.season(180, slope = 0.05)      \n",
    "temp = ts.decomp(season.trend, is.df = FALSE, Mult = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** In the cell below create and exectue the code to plot the ACF and PACF of the residual compontent for the STL decomposition. Does it appear that the STL process has removed the trend and seasonal components of the time series fully or partially? **Hint**, the residual in the column three of the data frame retured by `ts.decomp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Model for Random Walk\n",
    "\n",
    "How do we deal with a random walk component? Reeall that the random walk is modeled as a cumulative sum (or integral in the continuious case) over the innovations. Further, random walk series are **not stationary**. By takeing the differences (or derivatives in the continuious case) we can recover the innovations as a stationary white noise residual series.\n",
    "\n",
    "The code in the cell below using the `diff` function for time series to compute the difference in the random walk series. Execute this code and examine the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Use a first order difference series to \n",
    "## remove the trend\n",
    "ts.diff <- function(ts, lag = 1){\n",
    "  diff(ts, lag = lag)\n",
    "}\n",
    "diff.walk = ts.diff(ranWalk)\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "plot(diff.walk, main = 'Difference of random walk time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diference series resembles an iid white noise series. \n",
    "\n",
    "Run the code in the cell below to examine the distribution plots, the ACF and the PACF of the diference of the random walk series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "dist.ts(diff.walk, col = 'differance of random walk')\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=6)\n",
    "plot.acf(diff.walk, col = 'difference of random walk ', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the difference series values are close to Normal. The ACF and PACF plots verify that the difference series is a white noise series.  The differencing operator has transformed the non-stationary randowm walk series to a stationary white noise series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Models for the Residual Series\n",
    "\n",
    "Now that we have investigated the basic properties of time series and some decomposition methods, let's investigate models for dealing with the residuals.\n",
    "\n",
    "The class of models we will investigate are known and **autoregressive integrated moving average** or **ARIMA** model. We will work our way through each component of this model in the next few subsections. \n",
    "\n",
    "The ARIMA model and its relatives are **linear** in their coefficients. As you will see, the models are constrcted to to account for the serial correlations in time series data. At there code, these models are just special cases of linear regression. \n",
    "\n",
    "### Autoregressiive Model\n",
    "\n",
    "The values of an **autoregrissive** or **AR** time series are determined by a linear combination of the past values.In other words, the AR model accounts for serial correlation in the values of the time series. We can write the value of an autoregressive series or **order p** or **AR(p)** series at time t as:\n",
    "\n",
    "$$x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} \\dots \\alpha_p x_{t-p} + w_t$$\n",
    "\n",
    "An AR process has the following properties:\n",
    "\n",
    "- $\\rho_0 = 1$ always.\n",
    "- $p_k = \\alpha^k$\n",
    "- Number of nonzero PACF values = p.\n",
    "\n",
    "AR models are specifically for **stationary time series**. If the variance is not constant or the trend component has not been removed, AR models will not produce statisfactory results.\n",
    "\n",
    "Run the code in the cell below which simulates and plots an AR(1), or $x_t = 0.9\\ x_{t-1}$, model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ---- Simple ARMA models ------\n",
    "## Simulate an ARMA process\n",
    "arma.sim = function(ar = c(0.9), ma = c(0), n = 300, mean = 1.0){\n",
    "  ar1.model = list(ar = ar, ma = ma)\n",
    "  print(ar1.model)\n",
    "  ar1 = mean + arima.sim(model = ar1.model, n = n)\n",
    "  ar1\n",
    "}\n",
    "## --- AR(1) process\n",
    "arMod = arma.sim()\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "plot(arMod, main = 'Plot of AR(1) model time series')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series shows significant deviatiions from the zero, and a bit drift from zero. \n",
    "\n",
    "Run the code in the cell below to plot the ACF and PACF of the AR(1) series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(repr.pmales.extlot.width=8, repr.plot.height=6)\n",
    "plot.acf(arMod, col = 'AR(1) model', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AR(1) model produces a series with significant correlations in the lags, as shown in the  ACF plot. More importantly, the PACF has 1 significant lag value, consistent  with the AR(1) model. \n",
    "\n",
    "The AR time series model estimates the coeficients for the order of the model specified. The order of  the model is specified as `c(p,0,0)` in R vector. Run this code and examine  the  properties of the  results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Function for ARIMA model estimation\n",
    "ts.model = function(ts, col = 'remainder', order = c(0,0,1)){\n",
    "  mod = arima(ts, order = order, include.mean = FALSE)\n",
    "  print(mod)\n",
    "  mod\n",
    "}\n",
    "mod.est = ts.model(arMod, col = 'AR(1) process', order = c(1,0,0))\n",
    "plot.acf(mod.est$resid[-1], col = 'AR(1) estimate', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following about the AR model:\n",
    "\n",
    "- The estimated AR coefficient is nearly the same value as the orginal model. Further, the standard error is small relative to the value of the coefficient. \n",
    "- On the lag 0 value is  significant in the ACF  plot, indicating the estimate has removed the AR components from the original series. \n",
    "- The PACF remains insignificant, as is the case of the original series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Average Model\n",
    "\n",
    "For a **moving average** or **MA** model the value of the time series at time `t` is determined by a linear combination of past white noise terms. In other words, the MA model accounts for series correlation in the noise terms. We can write the MA(q) model as the linear combination of the last `q` white noise terms $w_i$:\n",
    "\n",
    "$$x_t = w_t + \\beta_1 w_{t-1} + \\beta_2 w_{t-2} + \\cdots + \\beta_q w_{t-q}$$\n",
    "\n",
    "An MA process has the following properties:\n",
    "\n",
    "- $\\rho_0 = 1$ always.\n",
    "- Number of nonzero $\\rho_k; k \\ne 0$ values = q.\n",
    "\n",
    "MA models are specifically for **stationary time series**. If the variance is not constant or the trend component has not been removed, MA models will not produce statisfactory results.\n",
    "\n",
    "The code in the cell below computes an `MA(1)` model with a coeficient $\\beta_1 = 0.9$, and plots the results. Run this code and examine these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## --- MA(1) process\n",
    "arMod = arma.sim(ar = c(0.001), ma = (0.9))\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "plot(arMod, main = 'Plot of MA(1) model time series')\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=6)\n",
    "plot.acf(arMod, col = 'MA(1) model', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected for an MA(1) process, the ACF has two significant values at lag 0 and 1. The PACF shows the results of the multiple serial correlations. \n",
    "\n",
    "Let's try to estimate the coefficients of the MA time series. The code in the cell below fits and MA(1) model to the time series. The model is specified as `c(0,0,q)` in R. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mod.est = ts.model(arMod, col = 'MA(1) process', order = c(0,0,1))\n",
    "plot.acf(mod.est$resid[-1], col = 'MA(1) estimate', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following about the MA model:\n",
    "\n",
    "- The estimated MA coefficient is nearly the same value as the orginal model. Further, the standard error is small relative to the value of the coefficient. \n",
    "- The PACF values are all insignificant, indicating the estimate has removed the MA components from the original series. \n",
    "- Only the lag 0 value is significant in the ACF  plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Autoregressive Moving Average Model\n",
    "\n",
    "We can combine the AR and MA models to create an **autoregressive moving averate** or **ARMA** model. This model accounts for serial correlation in both noise terms and values. We would expect we can write an ARMA model of order `(p,q)` as:\n",
    "\n",
    "$$x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} \\dots \\alpha_p x_{t-p} +\\\\\n",
    "w_t + \\beta_1 w_{t-1} + \\beta_2 w_{t-2} + \\cdots + \\beta_q w_{t-q}$$\n",
    "\n",
    "The code in the cell below simulates and plots an `MA(1,1)` model. The model is specified in R by the vector `c(p,0,q)`. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ---- ARMA(1,1) process\n",
    "arMod = arma.sim(ar = c(0.9), ma = (0.9))\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=4)\n",
    "plot(arMod, main = 'Plot of ARMA(1,1) model time series')\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=6)\n",
    "plot.acf(arMod, col = 'ARMA(1,1) model', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the ARMA(1,1) series has properties of both an AR(1) and MA(1) series. \n",
    "\n",
    "**Your Turn:** Try estimating the parameters of the time series, printing a summary of the model and plot the ACF and PACF of the residual. **Hint**, the ARMA(1,1) model is specified in R as `c(p,0,q)` or `c(1,0,1)`.  Run your code, examine the results and answer these questions to deterimine if your model is a good one: \n",
    "\n",
    "- How do the estimated MA and AR coefficients compare to the orginal model?\n",
    "- How do the standard errors of the coefficients compare to the magnitudes of the coefficients? \n",
    "- Are all PACF values insignificant, indicating the estimate has removed the MA components from the original series?\n",
    "- Is the lag 0 value the only significant value of the ACF, indicating the model has removed the AR component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive Integrated Moving Average Model\n",
    "\n",
    "The **autoregreessive integrated moving average**, or **ARIMA** model adds an integrating term to the ARMA model. The integrating component performs differencing to model a random walk component. The integrating component models one of the **non-stationary** parts of a time series. The ARIMA model is defined by orders p, d, q. We have already looked at AR(p) and MA(q) models. The order of the differencing operator of the integrating term is defined by `d`. Since the integrating term is a differencing operator, there is no coeficient to estimate. \n",
    "\n",
    "In a pervious section we have already simulated a randomw walk series, and investigated its properties. The code in the cell below estimates the parameters for and plots the results of an ARIMA(0,1,0) model. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## ARMA model of the random walk difference process \n",
    "arima.est = ts.model(ranWalk, col = 'ARIMA process model', order = c(0,1,0))\n",
    "cat(paste('Sigma squared of the original series = ', as.character(var(ranWalk))))\n",
    "plot.acf(arima.est$resid[-1], col = 'ARIMA(0.1,0) of random walk ', is.df = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show that the first order differrence operator has removed the random walk component of the time series. Specifically: \n",
    "\n",
    "- The $\\sigma^2$ or variance of the residual series is an order of magnitude less than the original series. \n",
    "- The PACF values are all insignificant, as expected from a white noise residual. \n",
    "- Only the lag 0 value is significant in the ACF, as expected from a white noise residual. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Data Example\n",
    "\n",
    "Let's apply the models we have been working with on some real-world data. We will work with a data set which shows the consumpton of chocolate, beer and electricity in Australia from 1958 to 1991. \n",
    "\n",
    "### Load and Examine the Data\n",
    "\n",
    "As a first step, run the code in the cell below to load the data from the `.csv` file and examine the head of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### ------- Real world data sets ------------\n",
    "#\n",
    "# --------Electricity, beer, and chocolate ----\n",
    "#\n",
    "# Load the data from the Internet\n",
    "#www = \"http://www.maths.adelaide.edu.au/emac2009/#Data/cbe.dat\"\n",
    "getwd()\n",
    "CBE = read.table('cbe.dat', sep =\"\", header = TRUE)\n",
    "head(CBE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next step we plot the three time series and their log transformed values. Execute the code in the cell below to create these plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elec.ts = ts(CBE[,3], start = 1958, freq = 12)\n",
    "lnelec.ts = log(ts(CBE[,3], start = 1958, freq = 12))\n",
    "beer.ts = ts(CBE[,2], start = 1958, freq = 12)\n",
    "lnbeer.ts = ts(log(CBE[,2]), start = 1958, freq = 12)\n",
    "choclate.ts = ts(CBE[,1], start = 1958, freq = 12)\n",
    "lnchoclate.ts = log(ts(CBE[,1], start = 1958, freq = 12))\n",
    "aus.ts = cbind(elec.ts, lnelec.ts, beer.ts, lnbeer.ts, choclate.ts, lnchoclate.ts)\n",
    "## First look at the series\n",
    "options(repr.pmales.extlot.width=8, repr.plot.height=8)\n",
    "plot(aus.ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notive the follwing properties of these time series. \n",
    "\n",
    "- Each of these time series has a significant trend.\n",
    "- Each of these time series have a noticable seasonal component.\n",
    "- The magnitude of the seasonal component increases with trend in the un-transformed time series. \n",
    "- The seasonal component of the log transformeed series has a nearly constant magnitude. \n",
    "\n",
    "These results indicate that an sTL decomposition is required. Further, a multiplicative (log transformed) STL model is preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STL Decomposition of Electric Time Series\n",
    "\n",
    "Lets do some analysis of the electric time series. In this case, we will use a **multiplicative model** since the magnitude of the seasonal component generally increases with incresing trend. \n",
    "\n",
    "Execute the code in the cell below to compute the STL decomposition of the time series and examine the  results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elect.decomp = ts.decomp(lnelec.ts, Mult = TRUE, is.df = FALSE, span = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the following about these results:\n",
    "\n",
    "- The periodic component looks reasonable, but may not be doing a complete job as evidenced by the remainder.\n",
    "- The removal of the trend component appears to be satisfactory.\n",
    "\n",
    "As a next step, compute and plot the ACF and PACF for the remainder series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "options(repr.pmales.extlot.width=8, repr.plot.height=6)\n",
    "plot.acf(elect.decomp[, 3], is.df = FALSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ACF and PACF exhibit both AR and MA behavior. However, there are signs of periodicity which the STL decomposition has not removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply ARIMA Model\n",
    "\n",
    "Now that we have an STL decomposition of the electric use time series, we can compute an ARIMA model for the residual. As a starting point we will use an ARIMA(2,1,2) model. Run the code in the cell below and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute ARIMA model of electric residual\n",
    "beer.arima = ts.model(elect.decomp[, 3], col = 'ARIMA model for electricity', order = c(2,1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error of these coefficients are of similar magnitude or larger than the coefficients themselves in 3 of 4 cases. This behavior indicates that the model is over fit or over parameterized.\n",
    "\n",
    "**Your Turn:** The order of the ARIMA model must be reduced. In the cell below test some lower order models to find a better fit to the residual of the electricity use time series. Examine the model performance, paying particular attention to the AIC, and SE of the coefficients. Plot the ACF and PACF of the residual for your best model. Does the residual indicate the fit of the model is good? Are there some problems with this model? **Hint**, your can test a series of models specified by, say, `c(2,1,1)`, `c(1,1,2)`, `c(2,0,2)`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Time Series\n",
    "\n",
    "The R `forecast` package provides some powerful extensions to the standard time series anaytics. In particular, as the name implies, the forecast package is has tools for automatically estimating optimal **seasonal ARIMA** models and using these models to make future forecasts.\n",
    "\n",
    "The `auto.arima` functon from the `forecast` package does the following:\n",
    "\n",
    "- Performs a foreward stepwise regression starting from maximum order of each coeficient to find the optimal seasonal ARIMA model. The opimization is performed usingthe Akaike Information Criteria (AIC) and the Baysian Informatiion Criteria (BIC).\n",
    "- The seasonal ARIMA model can be writen as ARIM(p,d,q)(P,D,Q). We have already explored the standard ARIMA model, ARIMA(p,d,q). The additional seasonal terms model seasonal and trend behavior: \n",
    "  - P is the order of the seasonal autoregressive model.\n",
    "  - D is the order of a differening or integrating operator, which models the trend.\n",
    "  - Q is the order of the seasonal moving average model.\n",
    "  \n",
    "The **Baysian Information Criteria** or **BIC** is closely related to the Akaike Information Criteria. The BIC was proposed by Gideon Schwarz in 1978, and is sometimes referred to as the Schwarz Information Criteria. The BIC weights the number of parameters in the model by the log of the number of observations. We can write the BIC as:\n",
    "\n",
    "$$BIC = ln(n)\\ k- 2\\ ln(\\hat{L})\\\\\n",
    "where\\\\\n",
    "\\hat{L} = the\\ likelihood\\ given\\ the\\ fitted\\ model\\ parmaters\\ \\hat\\theta = p(x| \\hat\\theta)\\\\\n",
    "x = observed\\ data\\\\\n",
    "k = number\\ of\\ model\\ parameters\\\\\n",
    "n = number\\ of\\ observations$$\n",
    "  \n",
    "The `auto.arima` model has a large number of argumets with the purpose of bouding the range of search of p, d, q, P, D, and Q. Execute the example code in the cell below and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require(forecast)\n",
    "fit.elect = auto.arima(aus.ts[, 'lnelec.ts'], max.p=3, max.q=3,\n",
    "                       max.P=2, max.Q=2, max.order=5, max.d=2, max.D=1,\n",
    "                       start.p=0, start.q=0, start.P=0, start.Q=0)\n",
    "summary(fit.elect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these results and notice the following points:\n",
    "\n",
    "- The estimated model is ARIMA(0,1,1)(2,1,1).\n",
    "- The standard errors of the model coeficients are generally an order of magnitude less than the values of the model coeficients. The exception being the `sar2` coeficient, which may not be that significant. \n",
    "- A number of error metrics are produced including:\n",
    "  - Mean error, ME.\n",
    "  - Root mean square error, or L2 error, RMSE.\n",
    "  - Mean absolute error, or L1 error, MAE.\n",
    "  - Mean prediction error, MPE, mean absolute prediction error, MAPE and mean absolute squarer error, or MASE. Prediction error is computed by itteratively updating the model in an expanding window and measuring the error of predicting the next value beyone the window.\n",
    "  - ACF1 is the correlation of the model residual at the first non-zero lag, or lag 1. \n",
    "  \n",
    "Next we can use the estimated optimal model to make predictions into the future. The code in the cell below computes, summarizes and plots a predicaiton or foecast for the next 12 months of electricity production in Australia. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Make the forecast for the next year\n",
    "elect.forecast = forecast(fit.elect, h=12)\n",
    "summary(elect.forecast)\n",
    "plot(elect.forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forecast looks reasonable and the 80% and 95% confidence Intervalues are relatively narrow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models for Non-Stationary Variance.\n",
    "\n",
    "The **Autoregressive conditional heteroskedasticity** or **ARCH** and **Generalized Autoregressive conditional heteroskedasticity** or **GARCH** model, and their many relatives, are specifically indended to deal with variance which changes with time. Robert Engle published the ARCH model in 1982 for which he was awarded the Nobel Prize in Economics in 2003. \n",
    "\n",
    "These mdoels are beyond the scope of this lesson. Additional information can be found in the references given earlier. Software packages for these models are widely available, including in R and Python.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Copyright 2017, Stephen F Elston. All right reserved. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
