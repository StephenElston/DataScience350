{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayes Models\n",
    "\n",
    "### Data Science 350\n",
    "### Stephen Elston\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Modern Baysian models are in the class of compuationally intensive models. Bayesian models are a rich class of models, which can provide attractive alternatives to frequentist models. \n",
    "\n",
    "![**A Baysian would win this bet**](img/Sun.png)\n",
    "**A Baysian would win this bet**\n",
    "\n",
    "## Brief history\n",
    "\n",
    "A restricted version of Bayes Theorem was proposed by Rev.Thomas Bayes (1702-1761). Bayes Theorem, was published postumously by his friend Richard Price. Bayes' interest was in probabilities of gambling games. He was also a supporter of Isac Newton's new theory of calculus, with his publication, &An Introduction to the Doctrine of Fluxions, and a Defence of the Mathematicians Against the Objections of the Author of The Analyst*.\n",
    "\n",
    "![](img/ThomasBayes.gif)\n",
    "\n",
    "A version of Bayes Theorem which in its modern form was published by Pierre-Simon Laplace in Essai philosophique sur les probabilitÃ©s 1814. Laplace applied Baysian methods to problems in celestial mechanics. These problems had great practical implicatons in the late 18th and early 19th centuries for the safe navigaton of ships. \n",
    " \n",
    "![](img/Laplace.jpg)\n",
    "\n",
    "The geophysicist and mathemtician Harold Jefferys extensively used Bayes' methods. His 1939 book, *The Theory of Probability* was in deliberate opposition to Fisher's methods using p-values.\n",
    "\n",
    "![](img/JeffreysProbability.jpg)\n",
    "\n",
    "\n",
    "Dispite the philosophical squables, Baysian methods endured and showed an increasing number of success stories. Pragmatists continued to use both approaches. A number of success during the Second World War, with the philosophical battles raging, included:\n",
    "\n",
    "- Bayesian models were used to improve artillery accuracy in both world wars. In particular the Soviet statistian Kolmagorov used Bayes methods to greatly improve artillery accuracy. \n",
    "- Bayesian models used by Alan Turing to break German codes.\n",
    "- Bernard Koopman, working for the British Royal Navy, improved the ability to locate U-boats using directional data from intercepted radio transmissions. \n",
    "\n",
    "\n",
    "Starting in the second half of the 20th century the convergance of greater computing power and general acceptance lead to the following notable advances in computational Baysian methods.\n",
    "\n",
    "- Statistical sampling using Monte Carlo methods; stanislaw ulam, John von Neuman; 1946, 1947\n",
    "- MCMC, or Markov Chain Monte Carlo; Metropolis et al. (1953) Journal of Chemical Physics\n",
    "- Hastings (1970), Monte Carlo sampling methods using Markov chains and their application\n",
    "- Geman and Geman (1984) Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images\n",
    "- Duane, Kennedy, Pendleton, and Roweth (1987),  Hamiltonian MCMC\n",
    "- Gelfand and Smith (1990), Sampling-based approaches to calculating marginal densities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Baysian vs. Frequentist Views\n",
    "\n",
    "The battle between Fisher, Jefferys and their protÃ©gÃ©s continued for most of the 20th century. This battle was bitter and often personal. The core of these argument were:\n",
    "\n",
    "- Fisher argued that the selection of a Bayesian prior distribution was purely subjective, allowing one to achieve any answer desired.\n",
    "- Jefferys argued that all knowledge is in fact subjective, and that choosing a confidence interval was subjective in any event.\n",
    "\n",
    "With greater compuational power and general acceptance, Bayes methods are now widely used in areas ranging from medical research to natural language understanding to web search. Amoung pragmatists, the common belief today is that some problems are better handled by Frequentist methods and some with Bayesian methods.\n",
    "\n",
    "Let's summarize the differences between the Baysian and Frequentist views. \n",
    "\n",
    "- Bayesian methods use priors to quantify what we know about parameters.\n",
    "- Frequentists do not quantify anything about the parameters, using p-values and confidence intervals to express the unknowns about parameters.\n",
    "\n",
    "Recalling that both views are useful, we can contrast these methods with a chart.\n",
    "\n",
    "![](img/FrequentistBayes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Some introductory texts\n",
    "\n",
    "These two books provide a broad and readable introduction to Baysian data analysis, well, sort of. Both books contain extensive examples using R and specialized Bayes packages.\n",
    "\n",
    "![](img/StatisticalRethinking.jpg)\n",
    "\n",
    "![](img/DoingBaysianDataAnalysis.jpg)\n",
    "\n",
    "### Modeling reference\n",
    "\n",
    "This book contains a comprehensive treatment of applying Baysian models. The level of treatments in intermediate. The examples are from the social sciences, but the methods can be applied more widely. The examples use R and specialized Bayes packages. \n",
    "\n",
    "![](img/BayesRegression.jpg)\n",
    "\n",
    "### Theory \n",
    "\n",
    "This book contains a comprehensive overview of the modern theory of Bayesian models. The book is at an advanced level. Only theory is addressed, which only very limited R code examples.  \n",
    "\n",
    "![](img/BaysianDataAnalysis.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorm\n",
    "\n",
    "Let's go through a simple derivation of Baye's theorm. Remember the rule for conditional probability:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B}{p(B)}\\\\\n",
    "And\\\\\n",
    "P(B|A) = \\frac{P(A \\cap B)}{p(A)}$$\n",
    "\n",
    "Eliminating $P(A \\cap B):$\n",
    "\n",
    "$$ P(B)P(A|B) = P(A)P(B|A) \\\\\n",
    "Or\\\\\n",
    "P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Which is Bayes' Theorm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example, medical test\n",
    "\n",
    "Let's work out the conditional probabilities for a medical test which has an accurcy of 99% for a very rare disease with a probability of occurance of 0.0003125. We can work out the contitonal probability tree as shown below.\n",
    "\n",
    "![](img/Medical.jpg)\n",
    "\n",
    "Let's apply Bayes Theorm to this problem. What we want to know is the chance someone actually has the disease given a positve test.\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)} \\\\\n",
    "P(Disease|Test+) = P(Test+|Disease) \\frac{P(Disease)}{P(Test+)}$$\n",
    "\n",
    "**Your Turn:** Compute the conditional probability of having the disease given a positive test. Hint, the probability of a positive test is the sum of the probability of a positive test given the disease and the probability of an erronious test given no disease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ".99 * 0.0003125 /(.99 * 0.0003125 + .01 * (1 - 0.0003125))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example, probabilities of eye and hair color\n",
    "\n",
    "A sample population has the following probabilities of eye and hair color combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eyeHair = data.frame(Black = c(0.11, 0.03, 0.03, 0.01), \n",
    "                     Brunette = c(0.2, 0.14, 0.09, 0.05),\n",
    "                     Red = c(0.04, 0.03, 0.02, 0.02),\n",
    "                     Blond = c(0.01, 0.16, 0.02, 0.03))\n",
    "row.names(eyeHair) = c('Brown', 'Blue', 'Hazel', 'Green')\n",
    "eyeHair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the table above are the conditmional probabilities. Note that in the case $P(hair|eye) = P(eye|hair)$. \n",
    "\n",
    "Given these probabilities joint probabilities, it is easy to compute the marginal probabilities by summing the probabilities in the rows and columns. THe **Marginal probability** is the probability of along one variable (one margin) of the distribution. For example, $P(Red)$ or $P(Green)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compute the marginal distribution of hair color and eye color\n",
    "eyeHair = rbind(eyeHair, apply(eyeHair, 2, sum))\n",
    "eyeHair$Marginal_eye = apply(eyeHair, 1, sum)\n",
    "row.names(eyeHair) = c('Brown', 'Blue', 'Hazel', 'Green', 'Marginal_hair')\n",
    "eyeHair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Use Bayes Theorm to compute the probability of each eye color given that the subject has blue eyes; $P(Hair\\ Color|Blue\\ Eyes)$. Hint, this is a bit tricky since $P(hair\\ color) = 1$ across all colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eyeHair['Blue', ] / eyeHair['Blue', 'Marginal_eye']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bayes Theorm\n",
    "\n",
    "We need a formulation of Bayes Theorm which is convienient to use for compuational problems. Specifically, we don't want to be stuck summing all of the possiblilites to compute $P(B)$. \n",
    "\n",
    "Look at some fun facts about conditional probabilities. \n",
    "\n",
    "$$\n",
    "ğ‘ƒ(ğµ \\cap A) = ğ‘ƒ(ğµâ”‚ğ´)ğ‘ƒ(ğ´) \\\\\n",
    "And \\\\\n",
    "ğ‘ƒ(ğµ)=ğ‘ƒ(ğµ \\cap ğ´)+ğ‘ƒ(ğµ \\cap \\bar{ğ´}) \\\\\n",
    "Then \\\\\n",
    "ğ‘ƒ(ğµ)=ğ‘ƒ(ğµâ”‚ğ´)ğ‘ƒ(ğ´)+ğ‘ƒ(ğµâ”‚ \\bar{ğ´})ğ‘ƒ(\\bar{ğ´}) \\\\\n",
    "where \\\\\n",
    "\\bar{A} = Not\\ A\n",
    "$$\n",
    "\n",
    "We can now rewrite Bayes Theorm:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{ğ‘ƒ(ğµâ”‚ğ´)ğ‘ƒ(ğ´)+ğ‘ƒ(ğµâ”‚ \\bar{ğ´})ğ‘ƒ(\\bar{ğ´})} \\\\ $$\n",
    "\n",
    "This is a bit of a mess. But fortunately, we don't always need the denominator, in which case we are just look at the un-normalized distribution:\n",
    "\n",
    "$$ğ‘ƒ(ğ´â”‚ğµ)=ğ‘˜âˆ™ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)$$\n",
    "\n",
    "Ignoring the normalizaton constant $k$, we get:\n",
    "\n",
    "$$ğ‘ƒ(ğ´â”‚ğµ) \\propto ğ‘ƒ(ğµ|ğ´)ğ‘ƒ(ğ´)$$\n",
    "\n",
    "### Applying the simplifed relationship Bayes Theorm\n",
    "\n",
    "How to we interpret the relationships shown above? We do this as follows:\n",
    "\n",
    "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
    "Or\\\\\n",
    "ğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ â”‚ğ‘‘ğ‘ğ‘¡ğ‘) \\propto ğ‘ƒ(ğ‘‘ğ‘ğ‘¡ğ‘|ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ )ğ‘ƒ(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿğ‘ ) $$\n",
    "\n",
    "These relationships apply to data distributions or to parameters in a model (partial slopes, intercept, error distributions, lasso constant,â€¦) and the observed data. \n",
    "\n",
    "### Creating Bayes models\n",
    "\n",
    "Given prior assumption about the behavior of the parameters (the prior), produce a model which tells us the probability of observing our data, to compute new probability of our parameters. Given this, the steps for working with a \n",
    "\n",
    "- Identify data relevant to the research question. E.g.: what are the measurement scales of the data?\n",
    "- Define a descriptive model for the data. For example, pick a linear model formula.\n",
    "- Specify a prior distribution of the parameters. For example, we think the error in the linear model is Normally distributed as $N(\\theta,\\sigma^2)$.\n",
    "- Use the Bayesian inference formula (above) to re-assess parameter probabilities.\n",
    "- Update if more data is observed. This is key! The posterior of a Baysian model naturally updates as more data is added, a form of learning.\n",
    "\n",
    "\n",
    "### How do we choose a prior?\n",
    "\n",
    "The choice of the prior is a serious problem when performing Bayesian analysis.In general, a prior must be convincing to a **sceptical audience**. Some possible approches include:\n",
    "\n",
    "- Prior observations\n",
    "- Domain knowledge\n",
    "- If poor knowledge use less informative prior\n",
    "- **Watch out:** A uniform prior is informative. For exampe, you must set the limits on range of values\n",
    " \n",
    "One analytically and compuationally simple choice is a **conjugate prior**. When a likelihood is multiplied by a conjugate prior the distribution of the postirior is the same as the likelihood. Most named distributions have conjugates. A few commonly used eamples are shown in the table below:\n",
    "\n",
    "Likelihood | Conjugate\n",
    "---|---\n",
    "Binomial|Beta\n",
    "Bernoulli|Beta\n",
    "Poisson|Gamma\n",
    "Categorical|Dirichlet\n",
    "Normal| Normal, Inverse Gama\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A First Example\n",
    "\n",
    "With a bit of theory in mind, let's pull things together with an example. In this example we consider the occurance of rain. The data are binomially distributed, it either rains on a day or it does not. In the example we will:\n",
    "\n",
    "- Select a prior\n",
    "- Using data compute the likelihood and posterior distributions. \n",
    "- Add more data to our data set to updated the posterior distribution.\n",
    "\n",
    "The posterior distribution in binomially distributed. We can write this formally for $k$ successes in $N$ trials:\n",
    "\n",
    "$$ P(A) = \\binom{N}{k} \\cdot p^kq^{N-k}$$\n",
    "\n",
    "Our prior should then be the probability distribution of $p$.\n",
    "\n",
    "### Choosing a prior\n",
    "\n",
    "Our first step in this analysis is to choose a prior distribution. Since our likelihood distribution is binomial, we will start with the conjugate Beta distribution. Formally, we can write the Beta distribution:\n",
    "\n",
    "$$Beta(p |a, b) = \\kappa x^{a-1}(1 - x)^{b-1} \\\\\n",
    "where,\\ \\kappa = normalization\\ constant$$\n",
    "\n",
    "The Beta distribution is define on the interval $0 \\le Beta(p|a,b) \\le 1$. The Beta distribution has two parameters, a and b, which determine the shape. To get a feel for the Beta distribution, exectute the code in the cell below which computes 100 examples on a 10x10 grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = c(0.5,1,2,3,4)\n",
    "beta = alpha\n",
    "x = seq(0.001,0.999,length=100)\n",
    "\n",
    "par(mfrow = c(5,5), mar=c(2,1,2,1)) # mar = c(bottom, left, top, right)\n",
    "sapply(alpha, function(a){\n",
    "  sapply(beta, function(b){\n",
    "    plot_title = paste(\"(a,b)=(\",a,\",\",b,\")\")\n",
    "    plot(x,dbeta(x,a,b),xlab=\"\",ylab=\"\",\n",
    "         main=plot_title, type=\"l\", lwd=2)\n",
    "  })\n",
    "})\n",
    "\n",
    "# Set plot options back to normal\n",
    "par(mar=c(5.1,4.1,4.1,2.1), mfrow=c(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the plots above, that the Beta distribution can take on quite a range of shapes, depending on the parameters. Generally if $a \\gt b$ the distribution skews to the rights, if $a \\lt b$ to the left, and symetric if $ a = b$.\n",
    "\n",
    "Let's say that we think the change of rain on any given day is 0.2, and that the probability at the 75% point is 0.28. We can compute a beta distribution for this prior using the `beta.select` function. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "library(LearnBayes)\n",
    "## I think the chance of rain is 0.2 with\n",
    "## with a probability at the 75% point of 0.28\n",
    "## Compute my Beta prior\n",
    "beta.par <- beta.select(list(p=0.5, x=0.2), list(p=0.75, x=.28))\n",
    "beta.par ## The parameters of my Beta distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the values of a and b which have been computed above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the posterior distribution\n",
    "\n",
    "At this point our model is just our prior distribution. If we want to estimate the chance of rain for the next two days, we just draw samples from this prior. In other words, without data, our posterior is just our prior.\n",
    "\n",
    "Run the code to see the relationship between the prior an posterior distributions in the absence of data. The `c(0,0)` indicates we have zero successes and zero failures; no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "require(repr)\n",
    "options(repr.plot.width=6, repr.plot.height=5)\n",
    "triplot(beta.par, c(0, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know which likelihood we will use with our prior distributions. Once we have some data we can update the posterior distribution. \n",
    "\n",
    "Let's say we observe 6 days of rain in the next 10. Update the model and observe the results by running the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta.par + c(6, 4)\n",
    "triplot(beta.par, c(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triple plot shows the distribution of the prior, the likelihood and the posterior. Notice that with addition of data the posterior is now between the prior and the likelihoood. \n",
    "\n",
    "If we observer the weather for another 10 days we see 5 days of rain. Run the code in the cell below to see how this additional data affects the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta.par + c(11, 9)\n",
    "triplot(beta.par, c(11, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior has moved closer to the likelihood once again. \n",
    "\n",
    "In another 20 days we observe 14 days with rain. Using this new data run the code in the cell below to update the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta.par + c(25, 15)\n",
    "triplot(beta.par, c(25, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect by now, adding more obervations to the model, moves the posterior closser to the likelihood. \n",
    "\n",
    "In fact, as data is added to a Bayesian model, the posterior moves toward the likelihood. This property has two important implications:\n",
    "\n",
    "- The prior matters less as more data is added to a Baysian model.\n",
    "- The inferences from Baysian and frequentist models tend to converge as data set sizes grow.\n",
    "\n",
    "**But, be careful!** With large scale problems with large numbers of parameters you may need enourmous data sets to see the convergance in behavior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** You arrive in distance city knowning nothing about the weather. You want to build a model to predict the likelihood of rain. Create a Bayesian model of the chance of rain by doing the following.\n",
    "\n",
    "- Since you know nothing about this city, choose an uninformative $Beta(1,1)$ distribution as you prior. \n",
    "- As before use a binomial likelihood\n",
    "- In your first 20 days in this city there are 2 rain days. Using this data compute your Baysian model and make the triplot. What does this tell you about the posterior distribution of the chances of rain?\n",
    "\n",
    "**Use new names for your variables!** If you do not, you will break the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation from the posterior distribution\n",
    "\n",
    "Once we have a posterior distribution we can simulate from this distribution for number of reasons. The simulation consists of taking a number of random draws from the posterior distribution. There are a number of reasons why you might want to simulate from the posterior distribution.\n",
    "\n",
    "- Compute credible or highest density intervals of the model parameters.\n",
    "- Test the model against the data.\n",
    "- Compute forecasts from the model.\n",
    "\n",
    "### Credible intervals\n",
    "\n",
    "A **credible interval** is an interval on the Baysian posterior distribution. The credible interval is sometime called the highest density interval (HDI), or highest posterior density interval. As an exxample, the 90% credible interval encompases the 90% of the posterior distribution with the highest probability desnsity.  \n",
    "\n",
    "The credible interval is the Baysian analog of the frequentist confidence interval. However, these two measures are conceptually different. The confidence interval is chosen on the distribution of a test statistic, whereas the credible interval is computed on the posterior distribution. For symetric distributions, for say loction parameters, the credible interval can be numerically the same as the confidence interval. However, in the general case, these two quantities can be quite different.\n",
    "\n",
    "The code in the cell below, plots the posterior distribution of the parameter of the binomial distribution parameter $p$. The 90% credible interval, or HDI, is also computed and displayed. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Simulate from the posterior and \n",
    "## compute confidence intervals\n",
    "options(repr.plot.width=8, repr.plot.height=5)\n",
    "beta.post.par <- beta.par + c(25, 15)\n",
    "post.sample <- rbeta(10000, beta.post.par[1], beta.post.par[2])\n",
    "par(mfrow = c(1,2))\n",
    "quants = quantile(post.sample, c(0.05, 0.95))\n",
    "breaks = seq(min(post.sample), max(post.sample), length.out = 41)\n",
    "hist(post.sample, breaks = breaks, \n",
    "     main = 'Distribution of samples \\n with 90% HDI',\n",
    "     xlab = 'Sample value',\n",
    "     ylab = 'Density')\n",
    "abline(v = quants[1], lty = 3, col = 'red', lwd = 3)\n",
    "abline(v = quants[2], lty = 3, col = 'red', lwd = 3)\n",
    "qqnorm(post.sample)\n",
    "par(mfrow = c(1,1))\n",
    "quants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model\n",
    "\n",
    "There are a number of ways to test a Baysian model by simulating from the posterior distribution. Data values computed from the simulated parameter values should resemble the data used to build the model. \n",
    "\n",
    "First, let's try a simpler test. We simply plot the maximum likelihood (frequentist) value of the parmeter $p$ on the distribution simulated from the posterior. This value, should lie well within the simulaated distribution. Run the code in the cell below and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Check on the model\n",
    "predplot(beta.post.par, 25, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum likelihood value of $p$ is near the center of the simulated values, so we are statisfied with the results of this test.\n",
    "\n",
    "### Forecasting\n",
    "\n",
    "We can forecast or predict likely future values of the data generating process, by computing values from parameter values simulated from the posterior distribution. Let's say we want to know the distribution of rainy days in the next 60 days. We can compute this distribution by simulating from the posterior, as in the code shown in the cell below. Run this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## What is the probability of observing 0-8 successes in the\n",
    "## next 60 trials?\n",
    "n <- 60\n",
    "s <- 0:n\n",
    "pred.probs <- pbetap(beta.post.par, n, s)\n",
    "plot(s, pred.probs, type=\"h\", \n",
    "     main = paste('Probability distribution of successes in', as.character(n), 'trials'),\n",
    "     xlab = 'Successes')\n",
    "discint(cbind(s, pred.probs), 0.90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Use the model you computed in the previous exercise to compute the distribution of rain days you can expect in the next 5 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](img/BayesDeNeon.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
