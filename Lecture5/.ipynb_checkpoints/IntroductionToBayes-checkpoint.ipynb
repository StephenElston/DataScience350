{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayes Models\n",
    "\n",
    "### Data Science 350\n",
    "### Stephen Elston\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Modern Baysian models are in the class of compuationally intensive models. Bayesian models are a rich class of models, which can provide attractive alternatives to frequentist models. \n",
    "\n",
    "![**A Baysian would win this bet**](img/Sun.png)\n",
    "**A Baysian would win this bet**\n",
    "\n",
    "## Brief history\n",
    "\n",
    "A restricted version of Bayes Theorem was proposed by Rev.Thomas Bayes (1702-1761). Bayes Theorem, was published postumously by his friend Richard Price. Bayes' interest was in probabilities of gambling games. He was also a supporter of Isac Newton's new theory of calculus, with his publication, *An Introduction to the Doctrine of Fluxions, and a Defence of the Mathematicians Against the Objections of the Author of The Analyst*.\n",
    "\n",
    "![](img/ThomasBayes.gif)\n",
    "\n",
    "Pierre-Simon Laplace published a version of Bayes Theorem, similar to its modern form, in his Essai philosophique sur les probabilités 1814. Laplace applied Baysian methods to problems in celestial mechanics. These problems had great practical implicatons in the late 18th and early 19th centuries for the safe navigaton of ships. \n",
    " \n",
    "![](img/Laplace.jpg)\n",
    "\n",
    "The geophysicist and mathemtician Harold Jefferys extensively used Bayes' methods. His 1939 book, *The Theory of Probability* was in deliberate opposition to Fisher's methods using p-values.\n",
    "\n",
    "![](img/JeffreysProbability.jpg)\n",
    "\n",
    "\n",
    "Dispite the philosophical squables, Baysian methods endured and showed an increasing number of success stories. Pragmatists continued to use both approaches. A number of success during the Second World War, with the philosophical battles raging, included:\n",
    "\n",
    "- Bayesian models were used to improve artillery accuracy in both world wars. In particular the Soviet statistian Kolmagorov used Bayes methods to greatly improve artillery accuracy. \n",
    "- Bayesian models were used by Alan Turing to break German codes.\n",
    "- Bernard Koopman, working for the British Royal Navy, improved the ability to locate U-boats using directional data from intercepted radio transmissions. \n",
    "\n",
    "\n",
    "Starting in the second half of the 20th century the convergance of greater computing power and general acceptance lead to the following notable advances in computational Baysian methods.\n",
    "\n",
    "- Statistical sampling using Monte Carlo methods; Stanislaw Ulam, John von Neuman; 1946, 1947\n",
    "- MCMC, or Markov Chain Monte Carlo; Metropolis et al. (1953) Journal of Chemical Physics\n",
    "- Hastings (1970), Monte Carlo sampling methods using Markov chains and their application\n",
    "- Geman and Geman (1984) Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images\n",
    "- Duane, Kennedy, Pendleton, and Roweth (1987),  Hamiltonian MCMC\n",
    "- Gelfand and Smith (1990), Sampling-based approaches to calculating marginal densities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Baysian vs. Frequentist Views\n",
    "\n",
    "The battle between Fisher, Jefferys and their protégés continued for most of the 20th century. This battle was bitter and often personal. The core of these argument were:\n",
    "\n",
    "- Fisher argued that the selection of a Bayesian prior distribution was purely subjective, allowing one to achieve any answer desired.\n",
    "- Jefferys argued that all knowledge is in fact subjective, and that choosing a confidence interval was subjective in any event.\n",
    "\n",
    "With greater compuational power and general acceptance, Bayes methods are now widely used in areas ranging from medical research to natural language understanding to web search. Amoung pragmatists, the common belief today is that some problems are better handled by Frequentist methods and some with Bayesian methods.\n",
    "\n",
    "Let's summarize the differences between the Baysian and Frequentist views. \n",
    "\n",
    "- Bayesian methods use priors to quantify what we know about parameters.\n",
    "- Frequentists do not quantify anything about the parameters, using p-values and confidence intervals to express the unknowns about parameters.\n",
    "\n",
    "Recalling that both views are useful, we can contrast these methods with a chart.\n",
    "\n",
    "![](img/FrequentistBayes.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Some introductory texts\n",
    "\n",
    "These two books provide a broad and readable introduction to Baysian data analysis. Well, sort of. Both books contain extensive examples using R and specialized Bayes packages.\n",
    "\n",
    "![](img/StatisticalRethinking.jpg)\n",
    "\n",
    "![](img/DoingBaysianDataAnalysis.jpg)\n",
    "\n",
    "### Modeling reference\n",
    "\n",
    "This book contains a comprehensive treatment of applying Baysian models. The level of treatments in intermediate. The examples are from the social sciences, but the methods can be applied more widely. The examples use R and specialized Bayes packages. \n",
    "\n",
    "![](img/BayesRegression.jpg)\n",
    "\n",
    "### Theory \n",
    "\n",
    "This book contains a comprehensive overview of the modern theory of Bayesian models. The book is at an advanced level. Only theory is addressed, which only very limited R code examples.  \n",
    "\n",
    "![](img/BaysianDataAnalysis.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorm\n",
    "\n",
    "Let's go through a simple derivation of Baye's theorm. Remember the rule for conditional probability:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A \\cap B)}{p(B)}\\\\\n",
    "and\\\\\n",
    "P(B|A) = \\frac{P(A \\cap B)}{p(A)}$$\n",
    "\n",
    "Eliminating $P(A \\cap B):$\n",
    "\n",
    "$$ P(B)P(A|B) = P(A)P(B|A) \\\\\n",
    "Or\\\\\n",
    "P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Which is Bayes' Theorm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example, probabilities of eye and hair color\n",
    "\n",
    "A sample population has the following probabilities of eye and hair color combinations. Execute thhe code to see the chart of conditional probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eyeHair = data.frame(Black = c(0.11, 0.03, 0.03, 0.01), \n",
    "                     Brunette = c(0.2, 0.14, 0.09, 0.05),\n",
    "                     Red = c(0.04, 0.03, 0.02, 0.02),\n",
    "                     Blond = c(0.01, 0.16, 0.02, 0.03))\n",
    "row.names(eyeHair) = c('Brown', 'Blue', 'Hazel', 'Green')\n",
    "eyeHair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure in the table above are the **conditmional probabilities**. Note that in the case: \n",
    "\n",
    "$$P(hair|eye) = P(eye|hair)$$ \n",
    "\n",
    "Given these conditional probabilities, it is easy to compute the marginal probabilities by summing the probabilities in the rows and columns. The **Marginal probability** is the probability along one variable (one margin) of the distribution. For example, $P(Red)$ or $P(Green)$. Like all probability distributions, marginal distributions must sum to 1.0. \n",
    "\n",
    "The code in the cell below computes the marginal probabilities by both hair color and eye color. Execute this code and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Compute the marginal distribution of hair color and eye color\n",
    "eyeHair = rbind(eyeHair, apply(eyeHair, 2, sum))\n",
    "eyeHair$Marginal_eye = apply(eyeHair, 1, sum)\n",
    "row.names(eyeHair) = c('Brown', 'Blue', 'Hazel', 'Green', 'Marginal_hair')\n",
    "eyeHair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn:** Use Bayes Theorm to compute the probability of each eye color given that the subject has blue eyes; $P(Hair\\ Color|Blue\\ Eyes)$. Hint, this is a bit tricky since $P(hair\\ color) = 1$ across all colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Bayes Theorm\n",
    "\n",
    "We need a formulation of Bayes Theorm which is convienient to use for compuational problems. Specifically, we don't want to be stuck summing all of the possiblilites to compute $P(B)$. \n",
    "\n",
    "Look at some fun facts about conditional probabilities. \n",
    "\n",
    "$$\n",
    "𝑃(𝐵 \\cap A) = 𝑃(𝐵|𝐴)𝑃(𝐴) \\\\\n",
    "And \\\\\n",
    "𝑃(𝐵)=𝑃(𝐵 \\cap 𝐴)+𝑃(𝐵 \\cap \\bar{𝐴}) \\\\\n",
    "Then \\\\\n",
    "𝑃(𝐵)=𝑃(𝐵|𝐴)𝑃(𝐴)+𝑃(𝐵│ \\bar{𝐴})𝑃(\\bar{𝐴}) \\\\\n",
    "where \\\\\n",
    "\\bar{A} = Not\\ A\n",
    "$$\n",
    "\n",
    "We can now rewrite Bayes Theorm:\n",
    "\n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{𝑃(𝐵│𝐴)𝑃(𝐴)+𝑃(𝐵│ \\bar{𝐴})𝑃(\\bar{𝐴})} \\\\ $$\n",
    "\n",
    "This is a bit of a mess. But fortunately, we don't always need the denominator. We can rewrite Bayes Theorem as:\n",
    "\n",
    "$$𝑃(𝐴│𝐵)=𝑘∙𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "Ignoring the normalizaton constant $k$, we get:\n",
    "\n",
    "$$𝑃(𝐴│𝐵) \\propto 𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "### Applying the simplifed relationship Bayes Theorm\n",
    "\n",
    "How to we interpret the relationships shown above? We do this as follows:\n",
    "\n",
    "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
    "Or\\\\\n",
    "𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠│𝑑𝑎𝑡𝑎) \\propto 𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) $$\n",
    "\n",
    "These relationships apply to the observed data distributions, or to parameters in a model (partial slopes, intercept, error distributions, lasso constant,…). \n",
    "\n",
    "### Creating Bayes models\n",
    "\n",
    "Given prior assumptions about the behavior of the parameters (the prior), produce a model which tells us the probability of observing our data, to compute new probability of our parameters. Given this, the steps for working with a \n",
    "\n",
    "- Identify data relevant to the research question. E.g.: what are the measurement scales of the data?\n",
    "- Define a descriptive model for the data. For example, pick a linear model formula.\n",
    "- Specify a prior distribution of the parameters. For example, we think the error in the linear model is Normally distributed as $N(\\theta,\\sigma^2)$.\n",
    "- Use the Bayesian inference formula (above) to compute posterior parameter probabilities.\n",
    "- Update if more data is observed. This is key! The posterior of a Baysian model naturally updates as more data is added, a form of learning.\n",
    "- Simulate data values from realizationns of the poterior distrion of the parameters.\n",
    "\n",
    "\n",
    "### How do we choose a prior?\n",
    "\n",
    "The choice of the prior is a serious problem when performing Bayesian analysis. In general, a prior must be convincing to a **sceptical audience**. Some possible approches include:\n",
    "\n",
    "- Prior observations\n",
    "- Domain knowledge\n",
    "- If poor knowledge use less informative prior\n",
    "- **Watch out:** A uniform prior is informative. For exampe, you must set the limits on range of values\n",
    " \n",
    "One analytically and compuationally simple choice is a **conjugate prior**. When a likelihood is multiplied by a conjugate prior the distribution of the postirior is the same as the likelihood. Most named distributions have conjugates. A few commonly used examples are shown in the table below:\n",
    "\n",
    "Likelihood | Conjugate\n",
    "---|---\n",
    "Binomial|Beta\n",
    "Bernoulli|Beta\n",
    "Poisson|Gamma\n",
    "Categorical|Dirichlet\n",
    "Normal| Normal, Inverse Gama\n",
    "\n",
    "However, there are many practical cases where a conjugate prior is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example\n",
    "\n",
    "Let's try a first example. \n",
    "\n",
    "With a bit of theory in mind, let's pull things together with an example. Let's say we are interested in analyzing districated drivers. We sample the behavior of 10 drivers at an intersection and determine if they exhibit distracted driving or not. The data are binomially distributed, a driver is distracted or not. In the example we will:\n",
    "\n",
    "- Select a prior for the parameter $p$, the probability of distracted driving.\n",
    "- Using data, compute the likelihood.\n",
    "- Compute the posterior and posterior distributions. \n",
    "- Try another prior distribution.\n",
    "- Add more data to our data set to updated the posterior distribution.\n",
    "\n",
    "The likelihood of the data and the posterior distribution are binomially distributed. The Binomial distribution has one parameter we need to estimate, $p$, the probability. We can write this formally for $k$ successes in $N$ trials:\n",
    "\n",
    "$$ P(A) = \\binom{N}{k} \\cdot p^k(1-p)^{N-k}$$\n",
    "\n",
    "The code in the cell below creates a simple data set of distracted drivers and computes some simple summary satistics. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drivers = c('yes','no','yes','no','no',\n",
    "          'yes','no','no','no','yes')\n",
    "distracted = ifelse(drivers == 'yes', 1, 0) # Convert to binary\n",
    "N = length(distracted)                  # sample size\n",
    "nDistracted = sum(distracted == 1)      # number of distracted drivers\n",
    "nNot = sum(distracted == 0)             # number not distracted\n",
    "cat(' Distracted drivers = ', nDistracted, 'Attentive drivers = ', nNot, \n",
    "    '\\n', 'Probability of distracted driving =', nDistracted / (nDistracted + nNot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test a prior distribution for our one model parameter $p$, $P(p)$. We don't know a lot about these drivers at this point, so we will start with a Uniform distribution. \n",
    "\n",
    "The code in the cell below computes and plots the uniform prior distribution. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "require(repr)\n",
    "options(repr.plot.width=6, repr.plot.height=5) ## Set the plot area\n",
    "\n",
    "N = 100\n",
    "p = seq(0.01, 0.99, length = N) \n",
    "pp = rep(1/N, length = N)\n",
    "plot(p, pp, typ = 'l', lwd = 2, col = 'blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to compute the likelihood. The likelihood is the probability of the data given the parameter, $P(X|p)$. We can view the observation of each driver as distracted or not as a Bernoulli trial, so we will use the Binomial distribution. \n",
    "\n",
    "The code in the cell below computes and plots the Binomial likelihood for the distracted driver data. This caluculation is performed for each value of $p$ we are  sampling. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "likelihood = function(p, dat){\n",
    "    k = sum(dat)\n",
    "    N = length(dat)\n",
    "    l = choose(N, k) * p^k * (1-p)^(N - k) # Compute Binomial likelihood\n",
    "    l / sum(l) # Normalize and return\n",
    "}\n",
    "l = likelihood(p, distracted)\n",
    "plot(p, l, , typ = 'l', lwd = 2, col = 'green', \n",
    "     ylab = 'Likelihood', xlab = 'Parameter',\n",
    "     main = 'Likelihood function')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a prior and a likelihood we are in a position to compute the posterior distribution of the parameter $p$, $P(p|X)$. The code in the cell below computes and plots the posterior, given the prior and likelihood.\n",
    "\n",
    "***\n",
    "**Warning!** The compuational methods used in this notebook are simplified for the purpose of illustration. For real-world problems, compuationally efficient code must be used!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "posterior = function(prior, like){\n",
    "    post = prior * like  # Compute the product of the probabilities\n",
    "    post / sum(post) # Normalize and return\n",
    "}\n",
    "\n",
    "plot.post = function(prior, like, post, x){\n",
    "    maxy = max(c(prior, like, post))\n",
    "    plot(x, like, , lty = 1, ylim = c(0.0, maxy), \n",
    "         ylab = 'Density', xlab = 'Parameter value',\n",
    "         main = 'Density of prior, likelihood, posterior',\n",
    "         lwd = 2, col = 'green')\n",
    "    lines(x, prior, lty = 2, lwd = 2, col = 'blue')    \n",
    "    lines(x, post, lty = 1, lwd = 2, col = 'red')\n",
    "    legend('topright', c('likelihood', 'prior', 'posterior'), \n",
    "    lty=1, col=c('green', 'blue', 'red'), bty='n', cex=1.0)\n",
    "    \n",
    "    cat(' Maximum of prior density =', round(x[which.max(prior)], 3), '\\n',\n",
    "        'Maximum likelihood =', round(x[which.max(like)], 3), '\\n',\n",
    "         'MAP =', round(x[which.max(post)], 3))\n",
    "}\n",
    "post = posterior(pp, l)\n",
    "plot.post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that with Uniform prior distribution, the posterior is just the likelihood. This is an important observation. The key point is that the frequentist probabilities are identical to the Bayesian posterior distribution given a Uniform prior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another prior\n",
    "\n",
    "Let's try another prior distribution. We will chose the **conjugate prior** of the Binomial distribution which is the Beta distribution. Formally, we can write the Beta distribution:\n",
    "\n",
    "$$Beta(p |a, b) = \\kappa x^{a-1}(1 - x)^{b-1} \\\\\n",
    "where,\\ \\kappa = normalization\\ constant$$\n",
    "\n",
    "The Beta distribution is define on the interval $0 \\le Beta(p|a,b) \\le 1$. The Beta distribution has two parameters, $a$ and $b$, which determine the shape. To get a feel for the Beta distribution, exectute the code in the cell below which computes 100 examples on a 10x10 grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=8) ## Set the plot area\n",
    "\n",
    "alpha = c(0.5,1,2,3,4)\n",
    "beta = alpha\n",
    "x = seq(0.001,0.999,length=100)\n",
    "\n",
    "par(mfrow = c(5,5), mar=c(2,1,2,1)) # mar = c(bottom, left, top, right)\n",
    "invisible(sapply(alpha, function(a){\n",
    "  sapply(beta, function(b){\n",
    "    plot_title = paste(\"(a,b)=(\",a,\",\",b,\")\")\n",
    "    plot(x,dbeta(x,a,b),xlab=\"\",ylab=\"\",\n",
    "         main=plot_title, type=\"l\", lwd=2)\n",
    "  })\n",
    "}))\n",
    "\n",
    "# Set plot options back to normal\n",
    "par(mar=c(5.1,4.1,4.1,2.1), mfrow=c(1,1))\n",
    "options(repr.plot.width=6, repr.plot.height=5) ## Set the plot area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the plots above, that the Beta distribution can take on quite a range of shapes, depending on the parameters. Generally if $a \\gt b$ the distribution skews to the rights, if $a \\lt b$ to the left, and symetric if $ a = b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still do not know a lot about the behavior of drivers, so we will pick a rather vague or broad Beta distribution as our prior. The code in the cell below uses a symetric prior with $a = 2$ and $b = 2$. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "beta.prior = function(x, a, b){\n",
    "    l = dbeta(x,a,b) # Compute Binomial likelihood\n",
    "    l / sum(l) # Normalize and return\n",
    "}\n",
    "pp = beta.prior(p, 2, 2)\n",
    "\n",
    "post = posterior(pp, l)\n",
    "plot.post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the mode of the posterior is close to the mode of the likelihood, but has shifted toward the mode of the prior. We call this tendancy of Bayesian posteriors to be shifted toward the prior the **shrinkage property**. The tendancy of the maximum likelihood point of the posterior is said to shrink toward the maximum likelihood point of the prior. \n",
    "\n",
    "We can now see that the posterior probability of distracted driving has a rather wide spread. How can we get a more definitive understanding of the probability of distracted driving?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding data to the Bayesian model\n",
    "\n",
    "Let's say that we observe some more drivers and gather some more data on distracted driving. Additional data will narrow the spread of the posterior distribution. As you might expect, adding more obervations to the model, moves the posterior closser to the likelihood. \n",
    "\n",
    "In fact, as data is added to a Bayesian model, the posterior moves toward the likelihood. This property has two important implications:\n",
    "\n",
    "- The prior matters less as more data is added to a Baysian model.\n",
    "- Adding data reduces shrinkage.\n",
    "- The inferences from Baysian and frequentist models tend to converge as data set sizes grow and the posterior approaches the likelihood.\n",
    "\n",
    "**But, be careful!** With large scale problems with large numbers of parameters you may need enourmous data sets to see the convergance in behavior. \n",
    "\n",
    "The code in the cell below adds another 10 observations to our data set. Execute this code and examine the results. How do the likelihood and posterior distributions compare with the case with only 10 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new.distracted = c('no','yes','no','no','no',\n",
    "          'yes','no','yes','no','no')  # Some new data\n",
    "new.distracted = ifelse(drivers == 'yes', 1, 0) # Convert to binary\n",
    "l = likelihood(p, c(distracted, new.distracted))\n",
    "post = posterior(pp, l)\n",
    "plot.post(pp, l, post, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credible Intervals\n",
    "\n",
    "A **credible interval** is an interval on the Baysian posterior distribution. The credible interval is sometime called the highest density interval (HDI), or highest posterior density interval (HPI). As an exxample, the 90% credible interval encompases the 90% of the posterior distribution with the highest probability desnsity.  \n",
    "\n",
    "The credible interval is the Baysian analog of the frequentist confidence interval. However, these two measures are conceptually different. The confidence interval is chosen on the distribution of a test statistic, whereas the credible interval is computed on the posterior distribution of the parameter. For symetric distributions the credible interval can be numerically the same as the confidence interval. However, in the general case, these two quantities can be quite different.  \n",
    "\n",
    "The code in the cell below, plots the posterior distribution of the parameter of the binomial distribution parameter  pp . The 95% credible interval, or HDI, is also computed and displayed. Execute this code and examine the result. \n",
    "\n",
    "***\n",
    "**Warning!** This code assumes a symetric prior distribution, so will not work in the general case. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nSamps = 100000\n",
    "qs = c(0.025, 0.975)\n",
    "\n",
    "plot.ci = function(p, post, nSamps, qs){\n",
    "    ## This function computes a credible interval using an assumption\n",
    "    ## of symetry in the bulk of the distribution to keep the \n",
    "    ## calculation simple. \n",
    "    ## Compute a large sample by resampling with replacement\n",
    "    samps = sample(p, size = nSamps, replace = TRUE, prob = post)\n",
    "    ci = quantile(samps, probs = qs) # compute the quantiles\n",
    "    \n",
    "    ## Plot the density with the credible interval\n",
    "    interval = qs[2] - qs[1]\n",
    "    title = paste('Posterior density with', interval, 'credible interval')\n",
    "    plot(p, post, , typ = 'l', ylab = 'Density', xlab = 'Parameter value',\n",
    "         main = title, lwd = 2, col = 'blue')\n",
    "    abline(v = ci[1], col = 'red', lty = 2, lwd = 2)\n",
    "    abline(v = ci[2], col = 'red', lty = 2, lwd = 2)\n",
    "    cat('The', interval, 'Credible interval is', \n",
    "        round(ci[1], 2), 'to', round(ci[2], 2))\n",
    "    }\n",
    "plot.ci(p, post, nSamps, qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating from the  posterior distribution: forecasting\n",
    "\n",
    "So far, we have computed the posterior distribution of the probability parameter $p$. But what about the distribution of distracted drivers? We can compute this distribution by simulating from the posterior distribution of $p$. \n",
    "\n",
    "The code in the cell below simulates and plots the distribution of distracted drivers. Run this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Num_cars = 10\n",
    "nSamps = 10000\n",
    "\n",
    "## We resample from the posterior distribution with replacecment, to get \n",
    "## a lage sample of posterior probabilities. \n",
    "## Next we scale by the number of cars in our forecast to get the probability\n",
    "## of each number of cars with distracted driving.\n",
    "## Finally, normalize to get the probabilities by number of cars with\n",
    "## distracted drivers.\n",
    "samps = round(Num_cars * sample(p, size = nSamps, replace = TRUE, prob = post))\n",
    "counts = table(samps) / nSamps\n",
    "#hist(samps)\n",
    "barplot(counts, \n",
    "        main = paste('Probability vs. number of distracted drivers in next', \n",
    "                     as.character(Num_cars)),\n",
    "        xlab = 'Number of distracted drivers', ylab = 'Probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Bayesian models\n",
    "\n",
    "How can we use Bayesian models to compare two distributions? It turns out that we can compare Bayesian models in severeal ways. In this lesson, we will compute and compare confidence intervals of the posterior distribution of a model parameter. \n",
    "\n",
    "For this example, we will compare the posterior distribution of the heights of sons to the heights of the mothers in the Galton Family dataset. As a first step, we will compute and evaluate Bayesian models for the mean heights using a subset of just 25 observations. \n",
    "\n",
    "The code in the cell below computes sub-samples the Galton family data and then plots histogram of the heights of sons and mothers. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "library(HistData)\n",
    "head(GaltonFamilies)\n",
    "require(dplyr)\n",
    "\n",
    "numSamps = 25 \n",
    "male = GaltonFamilies %>% filter(gender == 'male') %>% sample_n(numSamps)\n",
    "\n",
    "par(mfrow = c(2,1))\n",
    "hist(male$childHeight, main = 'Histograms of heights of people', \n",
    "     xlab = 'Height of  sons')\n",
    "hist(male$mother, main = '', xlab = 'Height of mothers')\n",
    "par(mfrow = c(1,1))\n",
    "meanHeight = round(mean(c(male$mother, male$father, male$childHeight)))\n",
    "cat('Mean of heights =', meanHeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this analysis, we need to select a prior distribution and compute the likelihood. First, we will address the likelihood. \n",
    "\n",
    "For these data, we will use a Normal likelihood. For a sample $X = {x_1, x_2, \\ldots, x_n}$, we can write the likelihood as:\n",
    "\n",
    "$$\n",
    "P(X | u, \\sigma) = \\bigg(\\frac{1}{2 \\pi \\sigma^2} \\bigg)^{\\frac{n}{2}} exp \\Bigg[ -\\frac{1}{2 \\sigma^2}  \\Bigg( \\sum_{i = 1}^n (x_i - \\bar{x})^2 + n(\\bar{x} - \\mu)^2 \\Bigg) \\Bigg] \\\\\n",
    "ignoring\\ constants\\ and\\ normalization\\\\\n",
    "\\propto exp \\bigg( -\\frac{n(\\bar{x} - \\mu)^2}{2 \\sigma^2} \\bigg) \n",
    "$$\n",
    "\n",
    "To simplify the compuations here, we will only estimate the posterior distribution of the mean. We will use a fixed emperical estimate of the standard deviation. A more complete analysis will also estimate the posterior distribution of the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N = 1000 \n",
    "p = seq(60.0, 75, length = N) \n",
    "\n",
    "male$childHeight = sort(male$childHeight, decreasing = FALSE)\n",
    "pp = dnorm(p, mean = meanHeight, sd = 5) ## start with a fairly broad prior\n",
    "pp = pp / sum(pp)\n",
    "\n",
    "comp.like = function(p, x){\n",
    "    l = rep(0, length = length(p))\n",
    "    sigmaSqr = sd(x)^2\n",
    "    xBar = mean(x)\n",
    "    cat(' Mean =', xBar, 'Standard deviation =', sqrt(sigmaSqr), '\\n')\n",
    "    n = length(x)\n",
    "#    l = sapply(p, function(u) dnorm(u, mean = xBar, sd = sigmaSqr))\n",
    "    l = sapply(p, function(u) exp(- n* (xBar - u)^2 / (2 * sigmaSqr)))\n",
    "    l / sum(l) # Normalize and return\n",
    "}\n",
    "\n",
    "    \n",
    "like.son = comp.like(p, male$childHeight)\n",
    "\n",
    "post.son = posterior(pp, like.son)\n",
    "plot.post(pp, like.son, post.son, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below computes the posterior distribution of the heights of the mothers. Run this code and examine the results. How do these results differ from the results for the hights of the sons?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "like.mom = comp.like(p, male$mother)\n",
    "\n",
    "post.mom = posterior(pp, like.mom)\n",
    "plot.post(pp, like.mom, post.mom, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the posterior distributions of the mean hights of the sons to the distribution of the mean heights of the mothers, we compute and compare the confidence intervals. \n",
    "\n",
    "Run the code  in the cell below which computes and plots the confidence intervals for the mean heights of the sons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nSamps = 100000\n",
    "qs = c(0.025, 0.975)\n",
    "plot.ci(p, post.son, nSamps, qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, compute and plot the posterior distribution and CIs of the mean of the heights of the mothers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot.ci(p, post.mom, nSamps, qs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the CIs for these posterior distributions. Are the distributions of the mean heights of sons and mothers significantly different?  \n",
    "\n",
    "***\n",
    "**Your turn:** Try the variations on the foregoing Bayesian analysis.\n",
    "\n",
    "1. The foregoing Bayesian analysis was performed with just 25 data points. Rerun this analysis with with 250 data points. How do the posterior distributions computed with the 250 data points compare to those computed with 25 data points? \n",
    "2. Perform the same analysis, except comparing the distributions of heights of sons and heights of fathers. \n",
    "\n",
    "**Important!:** Use another variable name for the likelihood and posterior for this exercise. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation from the posterior distribution\n",
    "\n",
    "Once we have a posterior distribution for parameters we can simulate from this distribution. The simulation consists of taking a number of random draws from the posterior parameter distribution and computing the posterior distribution of the data values. \n",
    "\n",
    "There are a number of reasons why you might want to simulate from the posterior distribution of data values.\n",
    "\n",
    "- Test the model against the data.\n",
    "- Compute forecasts of the dependent (label) variable from the model.\n",
    "\n",
    "The code in the cell below computes the posterior distribution of the hights of sons. This is done by computing the distribution of height based on realizations of the parameter (the mean) from the posterior distribution. The density distribution of the simulated heights is ploted along with the histogram of the original data. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sim.height = function(n, sigma, p, post){\n",
    "    ## Create probability weighted random same of values\n",
    "    ## of the mean height.\n",
    "    mu = sample(p, size = n, replace = TRUE, prob = post)\n",
    "    ## Now compute the values from the likelihood\n",
    "    rnorm(n, mu, sigma)\n",
    "}\n",
    "\n",
    "plot.dist = function(n, post, dat){\n",
    "    minx = min(c(post, dat))\n",
    "    maxx = max(c(post, dat))\n",
    "    hist(dat, prob = TRUE, xlim = c(minx, maxx),\n",
    "        xlab = 'Data value', ylab = 'Density', \n",
    "        main = 'Histogram of data with density of model predictions')\n",
    "    lines(density(sim.vals), col = 'red', lwd = 2)\n",
    "    }\n",
    "\n",
    "sim.vals = sim.height(10000, sigma = 2.616329, p = p, post = post.son)\n",
    "\n",
    "plot.dist(10000, sim.vals, male$childHeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine this chart. How well does the density of the posterior value distribution match the histogram of the original data values? Does the density of the posterior value distribtion deviate from Normal? \n",
    "\n",
    "***\n",
    "**Your turn:** Use the model you computed with 250 data values to create a plot like the one above. Examine these results and compare them to the results obtained with the model created with only 25 data values. Which posterior density function appears to represent the data better and why? Does the density deviate from Normal?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson, you have explored the following concepts:\n",
    "\n",
    "1. Application of Bayes Theorem.\n",
    "2. Computation of marginal distribtuions.\n",
    "3. Selection and computation of prior distributions.\n",
    "4. Selection and computation of likelihoods.\n",
    "5. Computation of posterior distributions.\n",
    "6. Computation and comparison of credible intervals. \n",
    "7. Simulation of data values from posterior distribution of model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](img/BayesDeNeon.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
